{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b091c81d",
   "metadata": {},
   "source": [
    "# Analyse Results\n",
    "\n",
    "This notebook contains the detailed analysis for all experiments for this bachelor project. It queries the evaluation data from W&B and displays/ visualises it nicely in `pd.DataFrames` (that are later compiled into LaTeX tables) and figures that are saved to the directory `reports/figures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import string\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "# retrieve data about runs\n",
    "import wandb\n",
    "\n",
    "# plotting and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7a7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format','{:.2f}'.format)\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "WANDB_PROJECT = \"mikasenghaas/bsc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b01b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import ImageDataset\n",
    "\n",
    "img = ImageDataset(**ImageDataset.default_config())\n",
    "class2id, id2class = img.class2id, img.id2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca584f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cbce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all runs\n",
    "api = wandb.Api()\n",
    "all_runs = api.runs(WANDB_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f4171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "def render_latex(df):\n",
    "    # capitalise col names\n",
    "    df.columns = [' '.join(map(lambda x: x[0].upper() + x[1:], col.split('_'))) for col in df.columns]\n",
    "    \n",
    "    # format df\n",
    "    s = df.style.highlight_max(props='bfseries: ;')\n",
    "    s.format(precision=2)\n",
    "    \n",
    "    # render latex\n",
    "    opts = {\"hrules\": True, \"position\": \"h\"}\n",
    "    return s.to_latex(**opts)\n",
    "\n",
    "def capitalise(s):\n",
    "    return s[0].upper() + s[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bab1806",
   "metadata": {},
   "source": [
    "# Experiment 1: Image and Video Classifiers\n",
    "\n",
    "This experiment compares a wide-variety of image and video classifiers. The classifiers tested are freely available on the PyTorch Hub. The following tables shows an overview over all models. We first filter all the runs that train models within the first experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f4a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = [run for run in all_runs if run.group == \"experiment1\"]\n",
    "\n",
    "print(f\"There are {len(runs)} runs in Experiment 1 ({', '.join([run.name for run in runs])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541292af",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Let's look at the training configuration setup of each model. This includes the hyperparameters for the optimser, learning rate scheduler and batch size and total number of trained epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c47d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert wandb config to df\n",
    "config = pd.DataFrame([dict(run.config, **{\"id\": run.name}) for run in runs])\n",
    "\n",
    "# drop unnecessary cols\n",
    "config = config.drop([\"model\", \"version\", \"wandb_name\", \"wandb_log\", \"wandb_tags\", \"all_classes\", \"first_floor\", \"ground_floor\", \"wandb_group\", \"include_classes\"], axis=1)\n",
    "\n",
    "# set index\n",
    "config = config.set_index(\"id\")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c6d127",
   "metadata": {},
   "source": [
    "Nothing suprising here. All hyperparameters are at the default values for the image and video classifiers. We can render to LaTex to put it into the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render latex\n",
    "training_params = [\"max_epochs\", \"batch_size\", \"lr\", \"gamma\", \"step_size\", \"device\"]\n",
    "data_params = [\"ratio\"]\n",
    "\n",
    "print(render_latex(config[training_params]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85f2ef",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The results section is responsible for plotting and saving all relevant figures that show the performance between the different models. We start by querying the summary statistics that were computed by the `eval.py` script and then synced to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b4f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert wandb summary to df\n",
    "all_summary = pd.DataFrame([dict({k: v for k, v in run.summary.items() if not k.startswith(\"_\")}, **{\"model_name\": run.name}) for run in runs])\n",
    "\n",
    "# choose relevant cols\n",
    "cols = [\"model_name\",\n",
    "        \"test_top1_acc\",\n",
    "        \"test_top3_acc\",\n",
    "        \"test_macro_f1\",\n",
    "        \"num_params\",\n",
    "        \"flops\",\n",
    "        \"samples_per_second_mean\",\n",
    "        \"samples_per_second_std\",\n",
    "       ]\n",
    "summary = all_summary[cols]\n",
    "\n",
    "summary[\"test_top1_acc\"] = summary[\"test_top1_acc\"] * 1e2 # to %\n",
    "summary[\"test_top3_acc\"] = summary[\"test_top3_acc\"] * 1e2 # to %\n",
    "summary[\"test_macro_f1\"] = summary[\"test_macro_f1\"] * 1e2 # to %\n",
    "summary[\"flops\"] = summary[\"flops\"] * 1e-9 # to %\n",
    "summary[\"num_params\"] = summary[\"num_params\"] * 1e-6 # to millions\n",
    "\n",
    "# add type of model with regex\n",
    "summary[\"type\"] = summary[\"model_name\"].apply(lambda x: \"video\" if re.search(r\"rnn|lstm\", x) else \"image\")\n",
    "summary[\"base\"] = summary[\"model_name\"].apply(lambda x: x.split(\"-\")[0])\n",
    "summary[\"head\"] = summary[\"model_name\"].apply(lambda x: \"None\" if x.find(\"-\") == -1 else x.split(\"-\")[1])\n",
    "\n",
    "# set index\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f9307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool(\"alexnet\".find(\"-\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f058160",
   "metadata": {},
   "source": [
    "We see the following:\n",
    "- Resnet18 is the best image classifier with (71.39% test top1-acc) and Resnet18-LSTM is the best classifier (72.19%) video classifier\n",
    "- MobileNetV3 struggled to learn anything meaningful (only 29% top1-acc)\n",
    "- Stacking a RNN/ LSTM layer does not seem to affect the performance a lot (slight increase/ decrease for ResNet, significant decrease for alexnet)\n",
    "- Top-3 Acc is generally higher than Top-1 Acc -> if the model is wrong, it's often almost right\n",
    "- Macro F1 is generally worse than Top1-Acc -> model focuses on getting the high-resource classes right\n",
    "- Larger model does not mean better performance here, resnet50 is double size but does not perform better, same with alexnet, however, mobilenet-v3 is really small and struggles to learn the true relationship\n",
    "- Throughput decreases with recurrent layer (less samples/ second), this makes these models less attractive\n",
    "- All models are valid in terms of throughput. Many can predict at a FPS rate of min. 30FPS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1280db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render latex\n",
    "print(render_latex(summary))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f2d5bae",
   "metadata": {},
   "source": [
    "Let's plot the Top-1 Accuracy, Top-3 Accuracy and Macro F1 score for each of the models in a barchart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aeff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first need to pivot the df\n",
    "tmp = summary[[\"model_name\", \"test_top1_acc\", \"test_top3_acc\", \"test_macro_f1\"]]\n",
    "tmp = tmp.melt(id_vars='model_name',\n",
    "         value_vars=['test_top1_acc', 'test_top3_acc', 'test_macro_f1'],\n",
    "         var_name='metric_type', value_name='metric_value')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e529a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked bar chart of three acc metrics for each model\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.barplot(\n",
    "    data=tmp,\n",
    "    x=\"model_name\", y=\"metric_value\", hue=\"metric_type\",\n",
    "    palette=\"Dark2\", ax=ax\n",
    ");\n",
    "\n",
    "# styling\n",
    "ax.set_ylim(0, 100) # set ylim max to 100\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=20) # rotate x ticks\n",
    "handles, labels = ax.get_legend_handles_labels() # get legend handles and labels\n",
    "ax.legend(title='Performance Metric', handles=handles, labels=[\"Top-1 Accuracy\", \"Top-3 Accuracy\", \"Macro F1 Score\"]);\n",
    "ax.set_xlabel(\"Model Name\", fontweight=\"bold\");\n",
    "ax.set_ylabel(\"Performance Metric (%)\", fontweight=\"bold\");\n",
    "\n",
    "fig.savefig(\"../report/figures/experiment1-performance-metricz.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa76b22b",
   "metadata": {},
   "source": [
    "Let's look at the relationship between some performance metrics. We want to make plots for:\n",
    "\n",
    "Performance vs. Complexity/ Efficiency\n",
    "- Top-1 Accuracy vs. Model Size\n",
    "- Top-1 Accuracy vs. GLOPS\n",
    "\n",
    "Other Performance vs. Complexity/ Efficiency\n",
    "- F1 vs. Model Size\n",
    "- F1 vs. GLOPS\n",
    "\n",
    "Performance\n",
    "- F1 vs. Model Size\n",
    "- F1 vs. GLOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(data, x, y, **kwargs):\n",
    "  \"\"\"\n",
    "  Scatter plot of summary metrics of models where hue is on the base and the \n",
    "  tip style is indicated by the type of RNN module that is appended\n",
    "\n",
    "  Args:\n",
    "    data (pd.DataFrame): summary dataframe\n",
    "    x (str): x-axis variable\n",
    "    y (str): y-axis variable\n",
    "  \n",
    "  Returns:\n",
    "    fig (matplotlib.axes.Figure): matplotlib figure object\n",
    "  \"\"\"\n",
    "  if kwargs.get(\"ax\"):\n",
    "    ax = kwargs.get(\"ax\")\n",
    "  else:\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "  sns.scatterplot(data=data, s=150, x=x, y=y, hue=\"base\", palette=\"Dark2\", style=\"head\", ax=ax)\n",
    "\n",
    "  # set x and y labels\n",
    "  if kwargs.get(\"xlabel\"):\n",
    "    ax.set_xlabel(kwargs.get(\"xlabel\"), fontweight=\"bold\");\n",
    "  if kwargs.get(\"ylabel\"):\n",
    "    ax.set_ylabel(kwargs.get(\"ylabel\"), fontweight=\"bold\");\n",
    "\n",
    "  # capitalise legend labels\n",
    "  handles, labels = ax.get_legend_handles_labels()\n",
    "  ax.legend(handles=handles, labels=list(map(capitalise, labels)), bbox_to_anchor=(1.05, 1));\n",
    "\n",
    "  # adding arrows to base-head pairs\n",
    "  for cnn in [\"alexnet\", \"resnet18\"]: # all models with a rnn head\n",
    "    xs = data.loc[data['model_name'] == cnn, x].values[0]\n",
    "    ys = data.loc[data['model_name'] == cnn, y].values[0]\n",
    "    for rnn in [\"rnn\", \"lstm\"]: # all rnn heads\n",
    "      xe = data.loc[data['model_name'] == f\"{cnn}-{rnn}\", x].values[0]\n",
    "      ye = data.loc[data['model_name'] == f\"{cnn}-{rnn}\", y].values[0]\n",
    "      ax.annotate(\"\", xy=(xe, ye), xytext=(xs, ys), arrowprops=dict(arrowstyle=\"->\", color=\"gray\"))\n",
    "    \n",
    "  # return for saving\n",
    "  if not kwargs.get(\"ax\"):\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204cd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of num params vs top-1 acc\n",
    "fig = scatter(summary, x=\"num_params\", y=\"test_top1_acc\", xlabel=\"Num. of Parameters (M)\", ylabel=\"Top-1 Accuracy (%)\")\n",
    "fig.savefig(\"../report/figures/experiment1-num-params-vs-top1-acc.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b76d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of flops vs top-1 acc\n",
    "fig = scatter(summary, x=\"flops\", y=\"test_top1_acc\", xlabel=\"FLOPs (G)\", ylabel=\"Top-1 Accuracy (%)\")\n",
    "fig.savefig(\"../report/figures/experiment1-flops-vs-top1-acc.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93138904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of flops vs top-1 acc\n",
    "fig = scatter(summary, x=\"samples_per_second_mean\", y=\"test_top1_acc\", xlabel=\"Throughput (Preds/s)\", ylabel=\"Top-1 Accuracy (%)\")\n",
    "fig.savefig(\"../report/figures/experiment1-throughput-vs-top1-acc.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6759da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all scatter plots into one plots\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(5*3, 4))\n",
    "xvars = [\"num_params\", \"flops\", \"samples_per_second_mean\"]\n",
    "xlabels = [\"Num. of Parameters (M)\", \"FLOPs (G)\", \"Throughput (Preds/s)\"]\n",
    "for xvar, xlabel, ax in zip(xvars, xlabels, axs):\n",
    "  scatter(summary, x=xvar, y=\"test_top1_acc\", xlabel=xlabel, ylabel=\"Top-1 Accuracy (%)\", ax=ax)\n",
    "for ax in axs[:-1]:\n",
    "  ax.get_legend().remove()\n",
    "\n",
    "fig.savefig(\"../report/figures/experiment1-scatter-plots.png\", dpi=300, bbox_inches=\"tight\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ac57c53",
   "metadata": {},
   "source": [
    "Let's look at the confusion matrix of each of the models. The confusion matrix is in the original summary dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the class names are two long, let's encode them as letters\n",
    "classes = class2id.keys()\n",
    "letters = [c for c in string.ascii_uppercase[:len(classes)]]\n",
    "class2letter = [{\"Class\": k, \"Encoding\": v} for k, v in zip(classes, letters)]\n",
    "\n",
    "print(render_latex(pd.DataFrame(class2letter).set_index(\"Class\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0426684",
   "metadata": {},
   "source": [
    "The confusion matrix was computed using `torcheval.metrics.multiclass_confusion_matrix(y_pred, y_true)`. From the docs we read:\n",
    "\n",
    "> Compute multi-class confusion matrix, a matrix of dimension num_classes x num_classes where each element at position (i,j) is the number of examples with true class i that were predicted to be class j. See also binary_confusion_matrix\n",
    "\n",
    "We can read the **Rows** denote the **True** Class and the Columns denotes the **Predicted** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac30cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = class2id.keys()\n",
    "letters = [c for c in string.ascii_uppercase[:len(classes)]]\n",
    "class2letter = {k: v for k, v in zip(classes, letters)}\n",
    "for record in all_summary[[\"model_name\", \"test_conf_matrix\"]].to_dict(orient=\"records\"):\n",
    "  model_name = record[\"model_name\"]\n",
    "\n",
    "  conf_matrix = np.array(record[\"test_conf_matrix\"])\n",
    "  recall_conf_matrix = (conf_matrix.T / conf_matrix.sum(1)).T # normalise by row, show recall\n",
    "  precision_conf_matrix = conf_matrix / conf_matrix.sum(0) # normalise by col, show precision\n",
    "\n",
    "  for title, mat in zip([\"conf-matrix\", \"recall-matrix\", \"precision-matrix\"], [conf_matrix, recall_conf_matrix, precision_conf_matrix]):\n",
    "    # set float precision to 2\n",
    "    mat = np.around(mat, decimals=2)\n",
    "    df_conf_matrix = pd.DataFrame(mat, index=letters, columns=letters)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(df_conf_matrix, annot=True, annot_kws={\"size\": 7}, ax=ax)\n",
    "    ax.set_xlabel(\"Predicted\", fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Actual\", fontweight=\"bold\")\n",
    "    # ax.set_title(f\"{title} for {model_name}\", fontweight=\"bold\")\n",
    "    fig.savefig(f\"../report/figures/experiment1-{title}-{model_name}.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "806efcbd",
   "metadata": {},
   "source": [
    "We can see the following:\n",
    "- Holes in the main diagonal even on good models, because the test split does not cover all classes (and showing actual count not weighted)\n",
    "- ResNet18 perform best overall. very nicely highlighted main diagonal.\n",
    "  - Base: \n",
    "  - RNN:\n",
    "  - LSTM:\n",
    "- ResNet50 is similar to ResNet50\n",
    "- mobilenet only ever predicts ground floor classes. that's odd? \n",
    "\n",
    "- All models struggle to differentiate GF Corridor 2 (K) and Atrium (M) -> predict majority class Atrium (M)\n",
    "- All models struggle to differentiate the different Libraries (D,E,F) -> library 1 often predicted as library 2, low precision for library 3\n",
    "- All models struggle to differentiate corridor 1 (L) and 2 (M) -> basically random choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4db4c0",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "\n",
    "This experiment compares different kinds of video classifiers against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0771d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = [run for run in all_runs if run.group == \"experiment2\"]\n",
    "\n",
    "print(f\"There are {len(runs)} runs in Experiment 2 ({', '.join([run.name for run in runs])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca53e4",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert wandb config to df\n",
    "config = pd.DataFrame([dict(run.config, **{\"id\": run.name}) for run in runs])\n",
    "\n",
    "# drop unnecessary cols\n",
    "config = config.drop([\"model\", \"version\", \"wandb_name\", \"wandb_log\", \"wandb_tags\", \"all_classes\", \"first_floor\", \"ground_floor\", \"wandb_group\", \"include_classes\"], axis=1)\n",
    "\n",
    "# set index\n",
    "config = config.set_index(\"id\")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd0585",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4956ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert wandb summary to df\n",
    "summary = pd.DataFrame([dict({k: v for k, v in run.summary.items() if not k.startswith(\"_\")}, **{\"id\": run.name}) for run in runs])\n",
    "\n",
    "# drop unnecessary cols\n",
    "summary = summary.drop([], axis=1)\n",
    "\n",
    "# set index\n",
    "summary = summary.set_index(\"id\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render latex\n",
    "print(render_latex(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1619b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in all_runs:\n",
    "    if run.id == \"n0itki9o\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b29014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert wandb summary to df\n",
    "summary = pd.DataFrame([dict(run.summary.items(), **{\"id\": run.name})])\n",
    "\n",
    "# drop unnecessary cols\n",
    "# summary = summary.drop([], axis=1)\n",
    "\n",
    "\n",
    "# set index\n",
    "summary = summary.set_index(\"id\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd8a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(render_latex(summary[[\"samples_per_second\"]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
