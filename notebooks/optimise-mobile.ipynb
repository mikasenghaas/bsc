{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5eb8df",
   "metadata": {},
   "source": [
    "## Optimisation for Mobile\n",
    "\n",
    "This notebook contains all tools to save and optimise a trained PyTorch model for mobile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f2e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348489e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "from config import *\n",
    "from utils import *\n",
    "from model import MODELS, FinetunedImageClassifier\n",
    "from transform import ImageTransformer\n",
    "from data import ImageDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadfefc4",
   "metadata": {},
   "source": [
    "## Load Trained Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e64be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which model to use\n",
    "MODEL = \"resnet18\"\n",
    "assert MODEL in MODELS, f\"Specified model has to be one of {list(MODELS.keys())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the paths of the most recently trained model\n",
    "model_path = os.path.join(MODEL_PATH, MODEL, f\"{MODEL}.pt\")\n",
    "config_path = os.path.join(MODEL_PATH, MODEL, \"config.json\")\n",
    "transforms_path = os.path.join(MODEL_PATH, MODEL, \"transforms.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load transform\n",
    "transform = load_pickle(transforms_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "config = load_json(config_path)\n",
    "class2id = config['class2id']\n",
    "id2class = {i:c for c,i in class2id.items()}\n",
    "model = FinetunedImageClassifier(**config)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075bb1ff",
   "metadata": {},
   "source": [
    "## Inference on Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test split and loader\n",
    "test_data = ImageDataset(split=\"test\", include_classes=list(class2id.keys()), ratio=1.0)\n",
    "test_loader = DataLoader(test_data, 16)\n",
    "\n",
    "# load batch of 16 images\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "# predict\n",
    "logits = model(transform(images))\n",
    "preds = logits.argmax(-1)\n",
    "\n",
    "# show images with ground truth\n",
    "show_images(images, titles=[f\"True: {id2class[t.item()]}\\nPred: {id2class[p.item()]}\" for t, p in zip(labels, preds)], show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d0852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on dummy\n",
    "dummy = torch.rand(3, 224, 224).unsqueeze(0)\n",
    "logits = model(dummy)\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b3fbf3",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "\n",
    "---\n",
    "\n",
    "From now on we are following the docs on [PyTorch Mobile](https://pytorch.org/mobile). PyTorch supports deploying trained machine learning models on mobile devices (by default: iOS and Android). This page summarises the necessary steps:\n",
    "\n",
    "When a PyTorch model is trained or retrained, or when a pre-trained model is available, for mobile deployment, follow the the recipes outlined in this summary so mobile apps can successfully use the model:\n",
    "\n",
    "1. **Fusing**. To fuse a list of PyTorch modules into a single module to reduce the model size before quantization, read the [Fuse Modules recipe](https://pytorch.org/tutorials/recipes/fuse.html).\n",
    "2. **Quantisation.** To reduce the model size and make it run faster without losing much on accuracy, read the [Quantization Recipe](https://pytorch.org/tutorials/recipes/quantization.html).\n",
    "3. **TorchScript.** To convert the model to TorchScipt and (optional) optimize it for mobile apps, read the [Script and Optimize for Mobile Recipe](https://pytorch.org/tutorials/recipes/script_optimized.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee725a",
   "metadata": {},
   "source": [
    "### Fusing\n",
    "\n",
    "---\n",
    "\n",
    "Following the [PyTorch Fusing Recipe](https://pytorch.org/tutorials/recipes/fuse.html): Model fusing is done before model quantisation. It describes the process of combining multiple PyTorch modules into a single modules to reduce its size and memory footprint. This may make the model **run faster** and **improve its accuracy**.\n",
    "\n",
    "_Note: Fusing is skipped at this point._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de102e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa62a7c",
   "metadata": {},
   "source": [
    "### Quantisation\n",
    "\n",
    "---\n",
    "\n",
    "Follwoing the [PyTorch Quantisation Recipe](https://pytorch.org/tutorials/recipes/quantization.html): Quantisation describes the process of converting model's weights and activations from a (default) 32-bit float representation to  to 8-bit ints. This process reduces the model's size to 1/4 (25%) of its original size and speeds up inference between 2-4x while maintaining equal or similar model accuracy.\n",
    "\n",
    "There are generally three approaches to use mobile quantisation:\n",
    "\n",
    "1. Use Pretrained Quantized Models: This approach is easy but only works for a subset of the models on Torchvision's Model Hub. There is support for `MobileNet v2`, `ResNet 18`, `ResNet 50`, `Inception v3`, `GoogleNet` and some more. \n",
    " \n",
    "2. Post Training Dynamic Quantisation: Not yet supported for convolutional layers in CNNs and therefore disregarded here.\n",
    " \n",
    "3. Post Training Static Quantisation: Convert all weights and activation to the smaller data type after training is completed. This approach is arguably the easiest to implement. \n",
    " \n",
    " \n",
    "4. Quantization-aware Training: Inserts fake quantisation to all weights and activations during training. Often used in CNN architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b211552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post training static quantisation\n",
    "\n",
    "model.eval()\n",
    "backend = \"qnnpack\" # for arm cpu (for x86 architectures, chooose 'fbgemm'\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "qmodel = torch.quantization.prepare(model, inplace=False)\n",
    "qmodel = torch.quantization.convert(qmodel, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db93ce51",
   "metadata": {},
   "source": [
    "## TorchScript\n",
    "\n",
    "---\n",
    "\n",
    "Following the [PyTorch Script and Optimize for Mobile Receip](https://pytorch.org/tutorials/recipes/script_optimized.html). To run a fused and quantised model in high-performance C++ environments (like iOS and Android), the model needs to be converted to `TorchScript` and can optionally be further optimised.\n",
    "\n",
    "There are two basic ways to convert a PyTorch model to TorchScript:\n",
    "\n",
    "1. The Trace Method: Uses a dummy input for the model. Only works if the model does not have any control flow.    \n",
    "\n",
    "2. The Script Method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d9443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torchscript\n",
    "torchscript_model = torch.jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a8562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "# optimise for mobile\n",
    "optimised_torchscript_model = optimize_for_mobile(torchscript_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2054edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to disk\n",
    "torchscript_model_path = os.path.join(MODEL_PATH, MODEL, f\"{MODEL}.pth\")\n",
    "\n",
    "optimised_torchscript_model.save(torchscript_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd6f0e",
   "metadata": {},
   "source": [
    "## Save as PyTorch Lite\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42011086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pytorch lite\n",
    "lite_model_path = os.path.join(MODEL_PATH, MODEL, f\"{MODEL}.ptl\")\n",
    "optimised_torchscript_model._save_for_lite_interpreter(lite_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ca786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare size of two model\n",
    "print(f\"Original Model Size: {round(os.path.getsize(model_path) / 1000 ** 2, 1)} MB\")\n",
    "print(f\"TorchScript Model Size: {round(os.path.getsize(torchscript_model_path) / 1000 ** 2, 1)} MB\")\n",
    "print(f\"PyTorch Lite Model Size: {round(os.path.getsize(lite_model_path) / 1000 ** 2, 1)} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
