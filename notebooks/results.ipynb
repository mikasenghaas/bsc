{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b091c81d",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "This notebook contains the detailed analysis for all experiments for this bachelor project. It queries the evaluation data from W&B and displays/ visualises it nicely in `pd.DataFrames` (that are later compiled into LaTeX tables) and figures that are saved to the directory `reports/figures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import os\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "# retrieve data about runs\n",
    "import wandb\n",
    "\n",
    "# plotting and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config import BASEPATH\n",
    "from defaults import DEFAULT\n",
    "from utils import show_images\n",
    "from transform import ImageTransform, VideoTransform\n",
    "from data import ImageDataset, VideoDataset\n",
    "from model import ImageClassifier, VideoClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7a7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format','{:.2f}'.format)\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "WANDB_PROJECT = \"mikasenghaas/bsc-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b01b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_config = DEFAULT[\"resnet18\"]\n",
    "video_config = DEFAULT[\"r2plus1d_18\"]\n",
    "\n",
    "image_transform = ImageTransform(**image_config[\"transform\"])\n",
    "video_transform = VideoTransform(**video_config[\"transform\"])\n",
    "image_data = ImageDataset(**image_config[\"dataset\"], split=\"train\", transform=image_transform)\n",
    "video_data = VideoDataset(**video_config[\"dataset\"], split=\"train\", transform=video_transform)\n",
    "\n",
    "assert image_data.class2id == video_data.class2id\n",
    "class2id = image_data.class2id\n",
    "id2class = image_data.id2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cbce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all runs\n",
    "api = wandb.Api()\n",
    "all_runs = api.runs(WANDB_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f4171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "def render_latex(df):\n",
    "    # capitalise col names\n",
    "    df.columns = [' '.join(map(lambda x: x[0].upper() + x[1:], col.split('_'))) for col in df.columns]\n",
    "    \n",
    "    # format df\n",
    "    s = df.style.highlight_max(props='bfseries: ;')\n",
    "    s.format(precision=2)\n",
    "    \n",
    "    # render latex\n",
    "    opts = {\"hrules\": True, \"position\": \"h\"}\n",
    "    return s.to_latex(**opts)\n",
    "\n",
    "def capitalise(s):\n",
    "    return s[0].upper() + s[1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bab1806",
   "metadata": {},
   "source": [
    "# Experiment Results\n",
    "\n",
    "This experiment compares a wide-variety of image and video classifiers. The classifiers tested are freely available on the PyTorch Hub. The following tables shows an overview over all models. We first filter all the runs that train models within the first experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f4a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(DEFAULT.keys())\n",
    "\n",
    "runs = [run for run in all_runs if run.name in models and run.state == \"finished\"]\n",
    "\n",
    "print(f\"There are {len(runs)} runs in Experiment 1 ({', '.join([run.name for run in runs])})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "541292af",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Let's look at the training configuration setup of each model. The setup is separated into the following categories, with the following keys\n",
    "\n",
    "- General: Model Name, Descripton, Link\n",
    "- Model: TorchHub Link, TorchHub Identifier, Pretrained, Num Classes \n",
    "- Data: Clip Duration (Video)\n",
    "- Loader: Batch Size, Shuffle\n",
    "- Optim: LR, Weight Decay\n",
    "- Trainer: Device, Epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c47d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general information\n",
    "general = pd.DataFrame([dict(run.config['general']) for run in runs])\n",
    "general.loc[3, \"name\"] = \"ViT B-16\"\n",
    "general.set_index(\"name\", inplace=True)\n",
    "general[\"type\"] = general[\"type\"].apply({\"image\": \"Single-Frame\", \"video\": \"Video\"}.get)\n",
    "general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a993f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model information\n",
    "model = pd.DataFrame([dict(run.config['model'], **{'name': run.config['general']['name']}) for run in runs]).set_index(\"name\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data information\n",
    "data = pd.DataFrame([dict(run.config['dataset'], **{'name': run.config['general']['name']}) for run in runs]).set_index(\"name\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader information\n",
    "loader = pd.DataFrame([dict(run.config['loader'], **{'name': run.config['general']['name']}) for run in runs]).set_index(\"name\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73591cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim information\n",
    "optim = pd.DataFrame([dict(run.config['optim'], **{'name': run.config['general']['name']}) for run in runs]).set_index(\"name\")\n",
    "optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52edf4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer information\n",
    "trainer = pd.DataFrame([dict(run.config['trainer'], **{'name': run.config['general']['name']}) for run in runs]).set_index(\"name\")\n",
    "trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c6d127",
   "metadata": {},
   "source": [
    "Nothing suprising here. All hyperparameters are at the default values for the image and video classifiers. We can render to LaTex to put it into the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85f2ef",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The results section is responsible for plotting and saving all relevant figures that show the performance between the different models. We start by querying the summary statistics that were computed by the `eval.py` script and then synced to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b4f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert wandb summary to df\n",
    "all_summary = pd.DataFrame([dict({k: v for k, v in run.summary.items() if not k.startswith(\"_\")}, **{\"Name\": run.config['general']['name']}) for run in runs])\n",
    "all_summary.loc[3, \"Name\"] = \"ViT B-16\"\n",
    "all_summary.set_index(\"Name\", inplace=True)\n",
    "all_summary.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc3e5c67",
   "metadata": {},
   "source": [
    "This is a lot of columns. Let's organise them into three categories:\n",
    "- Machine Specifications\n",
    "- Benchmarks (Model Size, FLOPS, Throughput, Latency)\n",
    "- Performance (Top1-Acc, Top3-Acc, Macro F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f670fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get machine info for benchmarking\n",
    "machine_info_cols = {\n",
    "    \"device\": \"Device\",\n",
    "    \"machine_info_system_system\": \"System\",\n",
    "    \"machine_info_system_release\": \"Release\",\n",
    "    \"machine_info_cpu_model\": \"CPU Model\",\n",
    "    \"machine_info_cpu_cores_physical\": \"CPU Cores\",\n",
    "    \"machine_info_cpu_frequency\": \"CPU Frequency\",\n",
    "    \"machine_info_gpus\": \"GPU\",\n",
    "    \"machine_info_memory_total\": \"Memory\"}\n",
    "\n",
    "benchmark_cols = {\n",
    "    \"params\": \"Params\",\n",
    "    \"flops\": \"FLOPs\",\n",
    "    \"frames_seen\": \"Frames Seen\",\n",
    "    \"timing_batch_size_1_on_device_inference_metrics_seconds_per_batch_mean\": \"Inference Time (Mean)\",\n",
    "    \"timing_batch_size_1_on_device_inference_metrics_batches_per_second_mean\": \"Inference Throughput (Mean)\",\n",
    "    \"timing_batch_size_1_on_device_inference_human_readable_batch_latency\": \"Inference Latency\",\n",
    "    \"timing_batch_size_1_on_device_inference_human_readable_batches_per_second\": \"Inference Throughput\",\n",
    "    }\n",
    "\n",
    "performance_cols = {\n",
    "    \"train/best_acc\": \"Train Top-1 Acc (Best)\",\n",
    "    \"val/best_acc\": \"Val Top-1 Acc (Best)\",\n",
    "    \"test_top1_acc\": \"Test Top-1 Acc\",\n",
    "    \"test_top3_acc\": \"Test Top-3 Acc\",\n",
    "    \"test_macro_f1\": \"Test Macro F1\",\n",
    "    \"test_conf_matrix\": \"Confusion Matrix\",\n",
    "    }\n",
    "\n",
    "machine_info = all_summary[machine_info_cols.keys()].rename(columns=machine_info_cols)\n",
    "benchmark = all_summary[benchmark_cols.keys()].rename(columns=benchmark_cols)\n",
    "performance = all_summary[performance_cols.keys()].rename(columns=performance_cols)\n",
    "\n",
    "# different units\n",
    "benchmark[\"Params\"] = benchmark[\"Params\"] / 1e6 # million\n",
    "benchmark[\"FLOPs\"] = benchmark[\"FLOPs\"] / 1e9 # gigaflops\n",
    "benchmark[\"Frames Seen\"] = benchmark[\"Frames Seen\"] / 1e3 # 1k frames\n",
    "benchmark[\"Inference Time (Mean)\"] = benchmark[\"Inference Time (Mean)\"] * 1e3 # to ms\n",
    "\n",
    "# different units\n",
    "performance[\"Train Top-1 Acc (Best)\"] = performance[\"Train Top-1 Acc (Best)\"] * 100 # %\n",
    "performance[\"Val Top-1 Acc (Best)\"] = performance[\"Val Top-1 Acc (Best)\"] * 100 # %\n",
    "performance[\"Test Top-1 Acc\"] = performance[\"Test Top-1 Acc\"] * 100 # %\n",
    "performance[\"Test Top-3 Acc\"] = performance[\"Test Top-3 Acc\"] * 100 # %\n",
    "performance[\"Test Macro F1\"] = performance[\"Test Macro F1\"] * 100 # %\n",
    "\n",
    "# add model type to benchmark\n",
    "benchmark[\"Model Type\"] = general[\"type\"]\n",
    "performance[\"Model Type\"] = general[\"type\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bda2a6bb",
   "metadata": {},
   "source": [
    "### Machine Specification\n",
    "\n",
    "These are the same for all models, so we can just query the specifications for any model. The benchmarking was performedn on the HPC at ITU, on a CPU with 20 cores at 3.3GHz, and 256GB of Memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e828dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_info.iloc[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "709885bb",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f649bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3750b83",
   "metadata": {},
   "source": [
    "### Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1010b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2404f76",
   "metadata": {},
   "source": [
    "## Performance and Efficiency (LaTeX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeoff = pd.concat([performance, benchmark], axis=1)\n",
    "tradeoff = tradeoff.drop(\"Model Type\", axis=1)\n",
    "tradeoff[\"Model Type\"] = general[\"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec468ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeoff_cols = [\n",
    "    \"Test Top-1 Acc\",\n",
    "    \"Test Top-3 Acc\",\n",
    "    \"Test Macro F1\",\n",
    "    \"FLOPs\",\n",
    "    \"Inference Latency\",\n",
    "    \"Inference Throughput\",\n",
    "]\n",
    "print(render_latex(tradeoff[tradeoff_cols]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01ded557",
   "metadata": {},
   "source": [
    "Let's plot the test performance metrics for all models. We first have to pivot the table, so that each metric is in a separate row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_performance = performance.reset_index()[[\"Name\", \"Test Top-1 Acc\", \"Test Top-3 Acc\", \"Test Macro F1\", \"Model Type\"]]\n",
    "pivoted_performance = pivoted_performance.melt(id_vars=[\"Name\", \"Model Type\"],\n",
    "         value_vars=[\"Test Top-1 Acc\", \"Test Top-3 Acc\", \"Test Macro F1\"],\n",
    "         var_name='Metric Type', value_name='Metric Value')\n",
    "\n",
    "# order by model type\n",
    "pivoted_performance = pivoted_performance.sort_values(by=[\"Model Type\", \"Metric Value\"])\n",
    "pivoted_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked bar chart of three acc metrics for each model\n",
    "# order = [\"MobileNet V3 Small\", \"EfficientNet V2 Small\", \"ViT B-16\", \"ResNet50\", \"AlexNet\", \"GoogleNet\", \"DenseNet121\", \"ResNet18\", \"X3D S\", \"R(2+1)D 18\"]\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=pivoted_performance,\n",
    "    x=\"Name\", y=\"Metric Value\", hue=\"Metric Type\",\n",
    "    palette=\"Greens\", width=0.7, ax=ax\n",
    ");\n",
    "\n",
    "# styling\n",
    "ax.set_ylim(0, 100) # set ylim max to 100\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30) # rotate x ticks\n",
    "ax.set_xlabel(\"Model Name\", fontweight=\"bold\");\n",
    "ax.set_ylabel(\"Performance Metric (%)\", fontweight=\"bold\");\n",
    "ax.axvline(7.5, color=\"gray\", linestyle=\"--\") # separate single-frame and video models\n",
    "\n",
    "# for metric, color in zip([\"Test Macro F1\", \"Test Top-1 Acc\", \"Test Top-3 Acc\"], [\"#CAE4C6\", \"#7DBA7F\", \"#00460A\"]):\n",
    "    # best_performance = performance[metric].max()\n",
    "    # ax.axhline(best_performance, color=color, linewidth=0.8, linestyle=\"--\")\n",
    "\n",
    "fig.savefig(os.path.join(BASEPATH, \"report\", \"figures\", \"performance-metrics.png\"), dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d632b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter of top-1 vs top-3 acc\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(7*3, 6))\n",
    "metrics = [\"Test Top-1 Acc\", \"Test Top-3 Acc\", \"Test Macro F1\"]\n",
    "combinations = list(itertools.combinations(metrics, 2))\n",
    "for pair, ax in zip(combinations, axs):\n",
    "    x, y = pair\n",
    "    sns.scatterplot(\n",
    "        data=performance,\n",
    "        x=x, y=y, hue=performance.index, style=\"Model Type\",\n",
    "        palette=\"Dark2\", s=120, ax=ax\n",
    "    );\n",
    "\n",
    "    # styling\n",
    "    ax.set_xlabel(f\"{x} (%)\", fontweight=\"bold\");\n",
    "    ax.set_ylabel(f\"{y} (%)\", fontweight=\"bold\");\n",
    "\n",
    "fig.savefig(os.path.join(BASEPATH, \"report\", \"figures\", \"performance-metrics-scatter.png\"), dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d710c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's investigate the difference between top-1 and top-3 acc\n",
    "performance[\"Test Top-3 Acc - Test Top-1 Acc\"] = performance[\"Test Top-3 Acc\"] - performance[\"Test Top-1 Acc\"]\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.barplot(\n",
    "    data=performance,\n",
    "    x=performance.index, y=\"Test Top-3 Acc - Test Top-1 Acc\",\n",
    "    palette=\"Dark2\", ax=ax\n",
    ")\n",
    "ax.set_xlabel(\"Model Name\", fontweight=\"bold\");\n",
    "ax.set_ylabel(\"Performance Gain (Top-3 - Top-1 Acc.)\", fontweight=\"bold\");\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=20); # rotate x ticks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f058160",
   "metadata": {},
   "source": [
    "We see the following:\n",
    "- Resnet18 is the best image classifier with (71.39% test top1-acc) and Resnet18-LSTM is the best classifier (72.19%) video classifier\n",
    "- MobileNetV3 struggled to learn anything meaningful (only 29% top1-acc)\n",
    "- Stacking a RNN/ LSTM layer does not seem to affect the performance a lot (slight increase/ decrease for ResNet, significant decrease for alexnet)\n",
    "- Top-3 Acc is generally higher than Top-1 Acc -> if the model is wrong, it's often almost right\n",
    "- Macro F1 is generally worse than Top1-Acc -> model focuses on getting the high-resource classes right\n",
    "- Larger model does not mean better performance here, resnet50 is double size but does not perform better, same with alexnet, however, mobilenet-v3 is really small and struggles to learn the true relationship\n",
    "- Throughput decreases with recurrent layer (less samples/ second), this makes these models less attractive\n",
    "- All models are valid in terms of throughput. Many can predict at a FPS rate of min. 30FPS "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc9140f6",
   "metadata": {},
   "source": [
    "\n",
    "## Performance/ Efficiency Trade-Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125fda34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter of top-1 acc vs model size, flops, inference time, inference throughput\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(7*2, 6))\n",
    "metrics = [\"Inference Time (Mean)\", \"Inference Throughput (Mean)\"]\n",
    "units = [\"ms\", \"Preds/s\"]\n",
    "\n",
    "# flatten 2d axs\n",
    "for i, ((x, unit), ax) in enumerate(zip(zip(metrics, units), axs)):\n",
    "    sns.scatterplot(\n",
    "        data=tradeoff,\n",
    "        x=x, y=\"Test Top-1 Acc\", hue=tradeoff.index, style=\"Model Type\",\n",
    "        palette=\"tab10\", s=120, ax=ax\n",
    "    );\n",
    "\n",
    "    # styling\n",
    "    ax.set_xlabel(f\"{x} ({unit})\", fontweight=\"bold\", fontsize=13);\n",
    "    if i % 2 == 0:\n",
    "        ax.set_ylabel(f\"Test Top-1 Acc (%)\", fontweight=\"bold\", fontsize=13); \n",
    "    else:\n",
    "        ax.legend_.remove()\n",
    "        ax.set_ylabel(\"\"); \n",
    "\n",
    "fig.savefig(os.path.join(BASEPATH, \"report\", \"figures\", \"performance-efficiency-tradeoff-scatter.png\"), dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75ec1087",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ac57c53",
   "metadata": {},
   "source": [
    "Let's look at the confusion matrix of each of the models. The confusion matrix is in the original summary dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the class names are two long, let's encode them as letters\n",
    "classes = class2id.keys()\n",
    "letters = [c for c in string.ascii_uppercase[:len(classes)]]\n",
    "class2letter = [{\"Class\": k, \"Encoding\": v} for k, v in zip(classes, letters)]\n",
    "class2letter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0426684",
   "metadata": {},
   "source": [
    "The confusion matrix was computed using `torcheval.metrics.multiclass_confusion_matrix(y_pred, y_true)`. From the docs we read:\n",
    "\n",
    "> Compute multi-class confusion matrix, a matrix of dimension num_classes x num_classes where each element at position (i,j) is the number of examples with true class i that were predicted to be class j. See also binary_confusion_matrix\n",
    "\n",
    "We can read the **Rows** denote the **True** Class and the Columns denotes the **Predicted** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b476f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac30cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "classes = class2id.keys()\n",
    "letters = [c for c in string.ascii_uppercase[:len(classes)]]\n",
    "class2letter = {k: v for k, v in zip(classes, letters)}\n",
    "models = [\"ResNet18\", \"R(2+1)D\"]\n",
    "for i, model in enumerate(models):\n",
    "  record = performance.loc[model, :]\n",
    "  model_type = record[\"Model Type\"]\n",
    "  conf_matrix = np.array(record[\"Confusion Matrix\"])\n",
    "  if model_type == \"Single-Frame\":\n",
    "    conf_matrix = conf_matrix / 1000 # convert to thousands\n",
    "  # recall_conf_matrix = (conf_matrix.T / conf_matrix.sum(1)).T # normalise by row, show recall\n",
    "  # precision_conf_matrix = conf_matrix / conf_matrix.sum(0) # normalise by col, show precision\n",
    "\n",
    "  conf_matrix = np.around(conf_matrix, 1)\n",
    "  conf_matrix = pd.DataFrame(conf_matrix, index=letters, columns=letters)\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(7, 6))\n",
    "  sns.heatmap(\n",
    "    conf_matrix, annot=True, linewidths=.5,\n",
    "    cmap=\"Greens\", annot_kws={\"size\": 7}, ax=ax\n",
    "  )\n",
    "  ax.set_xlabel(\"Predicted\", fontweight=\"bold\")\n",
    "  ax.set_ylabel(\"Actual\", fontweight=\"bold\")\n",
    "\n",
    "  corridors = patches.Rectangle((10, 10), 3, 3, linewidth=1, edgecolor='gray', facecolor='none')\n",
    "  libraries = patches.Rectangle((3, 3), 3, 3, linewidth=1, edgecolor='gray', facecolor='none')\n",
    "  ax.add_patch(corridors)\n",
    "  ax.add_patch(libraries)\n",
    "\n",
    "  # add top axis\n",
    "  topax = ax.secondary_xaxis(\"top\")\n",
    "  topax.set_xticks([5.5, 15.5], [\"First Floor\", \"Ground Floor\"], minor=True)\n",
    "  topax.set_xticks([10], [\"\"], minor=False)\n",
    "  topax.tick_params(which='minor', width=0)\n",
    "  topax.tick_params(which='major', width=1, length=20)\n",
    "  \n",
    "  fig.savefig(os.path.join(BASEPATH, \"report\", \"figures\", f\"conf-matrix-{model.replace(' ', '_').lower()}.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a748db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "  record = performance.loc[model, :]\n",
    "  model_type = record[\"Model Type\"]\n",
    "  conf_matrix = np.array(record[\"Confusion Matrix\"])\n",
    "  if model_type == \"Single-Frame\":\n",
    "    conf_matrix = conf_matrix / 1000 # convert to thousands\n",
    "  # recall_conf_matrix = (conf_matrix.T / conf_matrix.sum(1)).T # normalise by row, show recall\n",
    "  # precision_conf_matrix = conf_matrix / conf_matrix.sum(0) # normalise by col, show precision\n",
    "\n",
    "  conf_matrix = np.around(conf_matrix, 1)\n",
    "  conf_matrix = conf_matrix[[0, 2, 1], [0, 2, 1]]\n",
    "  conf_matrix = pd.DataFrame(conf_matrix, index=reversed(classes), columns=reversed(classes))\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(7, 6))\n",
    "  sns.heatmap(\n",
    "    conf_matrix, annot=True, linewidths=.5,\n",
    "    cmap=\"Greens\", annot_kws={\"size\": 7}, ax=ax\n",
    "  )\n",
    "  ax.set_xlabel(\"Predicted\", fontweight=\"bold\")\n",
    "  ax.set_ylabel(\"Actual\", fontweight=\"bold\")\n",
    "\n",
    "  corridors = patches.Rectangle((10, 10), 3, 3, linewidth=1, edgecolor='gray', facecolor='none')\n",
    "  libraries = patches.Rectangle((3, 3), 3, 3, linewidth=1, edgecolor='gray', facecolor='none')\n",
    "  ax.add_patch(corridors)\n",
    "  ax.add_patch(libraries)\n",
    "\n",
    "  # add top axis\n",
    "  topax = ax.secondary_xaxis(\"top\")\n",
    "  topax.set_xticks([5.5, 15.5], [\"First Floor\", \"Ground Floor\"], minor=True)\n",
    "  topax.set_xticks([10], [\"\"], minor=False)\n",
    "  topax.tick_params(which='minor', width=0)\n",
    "  topax.tick_params(which='major', width=1, length=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "806efcbd",
   "metadata": {},
   "source": [
    "We can see the following:\n",
    "- Holes in the main diagonal even on good models, because the test split does not cover all classes (and showing actual count not weighted)\n",
    "- ResNet18 perform best overall. very nicely highlighted main diagonal.\n",
    "  - Base: \n",
    "  - RNN:\n",
    "  - LSTM:\n",
    "- ResNet50 is similar to ResNet50\n",
    "- mobilenet only ever predicts ground floor classes. that's odd? \n",
    "\n",
    "- All models struggle to differentiate GF Corridor 2 (K) and Atrium (M) -> predict majority class Atrium (M)\n",
    "- All models struggle to differentiate the different Libraries (D,E,F) -> library 1 often predicted as library 2, low precision for library 3\n",
    "- All models struggle to differentiate corridor 1 (L) and 2 (M) -> basically random choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "003a14c5",
   "metadata": {},
   "source": [
    "## Mispredicted Samples\n",
    "\n",
    "To get a better feel for the characteristics of the two best performing models - the best performing image, and the best performing video classifier, I will load and look at 10 mispredicted samples for each model, and look at top three most confident predictions of the models.\n",
    "\n",
    "First, we need to get the 10 mispredicted instances for both models. For this we need to initialise and load the weights for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_performing_image, image_version = \"resnet18\", \"v11\"\n",
    "best_performing_video, video_version = \"r2plus1d_18\", \"v0\"\n",
    "\n",
    "image_config = DEFAULT[best_performing_image]\n",
    "video_config = DEFAULT[best_performing_video]\n",
    "\n",
    "image_transform = ImageTransform(**image_config[\"transform\"])\n",
    "video_transform = VideoTransform(**video_config[\"transform\"])\n",
    "\n",
    "image_test_data = ImageDataset(**image_config[\"dataset\"], split=\"test\", transform=image_transform)\n",
    "video_test_data = VideoDataset(**video_config[\"dataset\"], split=\"test\", transform=video_transform)\n",
    "\n",
    "image_test_loader = DataLoader(image_test_data, batch_size=1, shuffle=True)\n",
    "video_test_loader = DataLoader(video_test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c24a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise model architectures\n",
    "image_clf = ImageClassifier(**image_config[\"model\"])\n",
    "video_clf = VideoClassifier(**video_config[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a4b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights\n",
    "def download_artifact(model, version, project=\"bsc-2\"):\n",
    "  api = wandb.Api()\n",
    "  artifact_path = f\"mikasenghaas/{project}/{model}:{version}\"\n",
    "  artifact = api.artifact(artifact_path, type=\"model\")\n",
    "\n",
    "  filepath = os.path.join(BASEPATH, \"artifacts\", f\"{model}:{version}\")\n",
    "  artifact.download(root=filepath)\n",
    "  \n",
    "  model_path = os.path.join(filepath, f\"{model}.pt\")\n",
    "  return model_path\n",
    "    \n",
    "image_path = download_artifact(best_performing_image, image_version)\n",
    "video_path = download_artifact(best_performing_video, video_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd19ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_clf.load_state_dict(torch.load(image_path))\n",
    "video_clf.load_state_dict(torch.load(video_path))\n",
    "\n",
    "image_clf.eval()\n",
    "video_clf.eval()\n",
    "\n",
    "print(\"Loaded Weights\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24cb1183",
   "metadata": {},
   "source": [
    "Let's make an example prediction on the first frame and first video clip from the test data set to get an impression on how to use the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026bed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on first frame\n",
    "from utils import show_image\n",
    "\n",
    "frame, label_id = next(iter(image_test_loader))\n",
    "logits = image_clf(frame)\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "prob, pred = torch.max(probs, dim=1)\n",
    "\n",
    "show_image(frame.squeeze(), title=f\"Pred: {id2class[pred.item()]} ({round(prob.item()*100,2)}%)\\nTrue: {id2class[label_id.item()]}\", unnormalise=True, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c84d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on first clip\n",
    "import io\n",
    "import ipywidgets as widgets\n",
    "\n",
    "sample = next(iter(video_test_loader))\n",
    "video = sample[\"video\"]\n",
    "label = sample[\"label\"]\n",
    "\n",
    "logits = video_clf(video)\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "prob, pred = torch.max(probs, dim=1)\n",
    "\n",
    "def display_video(video, title, config):\n",
    "  mean = np.array(config[\"transform\"][\"mean\"])\n",
    "  std = np.array(config[\"transform\"][\"std\"])\n",
    "  video = video.permute(1,0,2,3)\n",
    "  video_widget = widgets.Image(format='jpeg')\n",
    "\n",
    "  # display the widget\n",
    "  display(video_widget)\n",
    "  for frame in video:\n",
    "    img = plt.imshow(np.array(((frame * std[:, None, None] + mean[:, None, None]) * 255.0).permute(1,2,0), dtype=np.uint8))\n",
    "    plt.title(title)\n",
    "    buffer = io.BytesIO()\n",
    "    plt.savefig(buffer, format='jpeg')\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    video_widget.value = buffer.getvalue()\n",
    "\n",
    "display_video(video.squeeze(0), f\"Pred: {id2class[pred.item()]} ({round(prob.item() * 100, 2)}%)\\nTrue: {label[0]}\", video_config) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2de8e4a",
   "metadata": {},
   "source": [
    "Cool! Let's gather ten mispredicted samples for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mispredicted samples for image classifier\n",
    "mispredicted_frames = []\n",
    "for x, y in image_test_loader:\n",
    "  logits = image_clf(x)\n",
    "  probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "  prob, pred = torch.max(probs, dim=1)\n",
    "  argsorted = torch.argsort(probs, dim=1)\n",
    "  top3_preds = torch.arange(20)[argsorted[0][-3:]] # get top 3 preds\n",
    "  top3_probs = probs[0][argsorted[0][-3:]] # get top 3 probs\n",
    "  if pred.item() != y.item():\n",
    "    mispredicted_frames.append({\n",
    "      \"frame\": x,\n",
    "      \"pred\": [id2class[pred.item()] for pred in top3_preds],\n",
    "      \"prob\": [prob.item() for prob in top3_probs],\n",
    "      \"true\": id2class[y.item()]\n",
    "      })\n",
    "  if len(mispredicted_frames) == 9:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85b3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mispredicted samples for video classifier\n",
    "mispredicted_clips = []\n",
    "for sample in video_test_loader:\n",
    "  x = sample[\"video\"]\n",
    "  y = sample[\"label\"][0]\n",
    "  logits = video_clf(x)\n",
    "  probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "  prob, pred = torch.max(probs, dim=1)\n",
    "  argsorted = torch.argsort(probs, dim=1)\n",
    "  top3_preds = torch.arange(20)[argsorted[0][-3:]] # get top 3 preds\n",
    "  top3_probs = probs[0][argsorted[0][-3:]] # get top 3 probs\n",
    "  if pred.item() != class2id[y]:\n",
    "    mispredicted_clips.append({\n",
    "      \"clip\": x,\n",
    "      \"pred\": [id2class[pred.item()] for pred in top3_preds],\n",
    "      \"prob\": [prob.item() for prob in top3_probs],\n",
    "      \"true\": y\n",
    "      })\n",
    "  if len(mispredicted_clips) == 9:\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "415d0da4",
   "metadata": {},
   "source": [
    "Let's have a look at the mispredicted samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4026764",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "titles = []\n",
    "for i, frame in enumerate(mispredicted_frames):\n",
    "  frames.append(frame[\"frame\"])\n",
    "  title = \"\"\n",
    "  for i, (pred, prob) in enumerate(zip(reversed(frame[\"pred\"]), reversed(frame[\"prob\"]))):\n",
    "    title += f\"{i+1}: {pred} ({round(prob * 100, 2)}%)\\n\"\n",
    "  title += f\"T: {frame['true']}\"\n",
    "  titles.append(title)\n",
    "  if i == 3:\n",
    "    break\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "# flatten 2d array\n",
    "axs = axs.flatten()\n",
    "mean = np.array(image_config[\"transform\"][\"mean\"])\n",
    "std = np.array(image_config[\"transform\"][\"std\"])\n",
    "for ax, img, txt in zip(axs, frames, titles):\n",
    "  # unnormalise\n",
    "  img = img.squeeze()\n",
    "  img = img * std[:, None, None] + mean[:, None, None]\n",
    "\n",
    "  ax.imshow((img.permute(1,2,0) * 255.0).numpy().astype(np.uint8))\n",
    "  ax.plot([], [], ' ', label=txt)\n",
    "  ax.legend(loc=\"lower left\", fontsize=12)\n",
    "  ax.axis('off')\n",
    "\n",
    "fig.tight_layout(pad=0.5)\n",
    "fig.savefig(os.path.join(BASEPATH, \"report\", \"figures\", \"resnet18-mispredicted-frames.png\"), dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91ae666b",
   "metadata": {},
   "source": [
    "Analysis of mispredicted clips:\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "titles = []\n",
    "for i, clip in enumerate(mispredicted_clips):\n",
    "  first_frame = clip[\"clip\"].squeeze().permute(1,0,2,3)[0].squeeze()\n",
    "  frames.append(first_frame)\n",
    "  title = \"\"\n",
    "  for i, (pred, prob) in enumerate(zip(reversed(clip[\"pred\"]), reversed(clip[\"prob\"]))):\n",
    "    title += f\"{i+1}: {pred} ({round(prob * 100, 2)}%)\\n\"\n",
    "  title += f\"T: {frame['true']}\"\n",
    "  titles.append(title)\n",
    "  if i == 3:\n",
    "    break\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "# flatten 2d array\n",
    "axs = axs.flatten()\n",
    "mean = np.array(video_config[\"transform\"][\"mean\"])\n",
    "std = np.array(video_config[\"transform\"][\"std\"])\n",
    "for ax, img, txt in zip(axs, frames, titles):\n",
    "  # unnormalise\n",
    "  img = img.squeeze()\n",
    "  img = img * std[:, None, None] + mean[:, None, None]\n",
    "\n",
    "  ax.imshow((img.permute(1,2,0) * 255.0).numpy().astype(np.uint8))\n",
    "  ax.plot([], [], ' ', label=txt)\n",
    "  ax.legend(loc=\"lower left\", fontsize=12)\n",
    "  ax.axis('off')\n",
    "\n",
    "fig.tight_layout(pad=0.5)\n",
    "fig.savefig(os.path.join(BASEPATH, \"report\", \"figures\", \"r2plus1d-mispredicted-frames.png\"), dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
