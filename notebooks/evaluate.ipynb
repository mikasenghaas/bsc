{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5eb8df",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "This notebook contains a wide variety of methods to evaluate a trained model that was logged to this public [W&B Experiment](https://wandb.ai/mikasenghaas/bsc?workspace=user-mikasenghaas). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f2e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348489e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# custom scripts\n",
    "from config import *\n",
    "from utils import *\n",
    "from model import MODELS, FinetunedImageClassifier\n",
    "from transform import ImageTransformer\n",
    "from data import ImageDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadfefc4",
   "metadata": {},
   "source": [
    "## Load Trained Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e64be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which model to use\n",
    "MODEL = \"resnet18\"\n",
    "VERSION = \"v1\"\n",
    "assert MODEL in MODELS, f\"Specified model has to be one of {list(MODELS.keys())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the artifacts from the wandb server\n",
    "import wandb\n",
    "\n",
    "SAVE_PATH = path = os.path.join(BASEPATH, \"artifacts\", f\"{MODEL}:{VERSION}\")\n",
    "\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact(f'mikasenghaas/bsc/{MODEL}:{VERSION}', type='model')\n",
    "relative_path = artifact.download(root=SAVE_PATH)\n",
    "\n",
    "print(f\"{MODEL}:{VERSION} downloaded to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the paths of the most recently trained model\n",
    "model_path = os.path.join(SAVE_PATH, f\"{MODEL}.pt\")\n",
    "config_path = os.path.join(SAVE_PATH, \"config.json\")\n",
    "transforms_path = os.path.join(SAVE_PATH, \"transforms.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load transform\n",
    "transform = load_pickle(transforms_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "config = load_json(config_path)\n",
    "class2id = config['class2id']\n",
    "id2class = {i:c for c,i in class2id.items()}\n",
    "model = FinetunedImageClassifier(**config)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075bb1ff",
   "metadata": {},
   "source": [
    "## Prediction Examples on Test Split\n",
    "\n",
    "We sample a random batch of `16` frames from the dataset and visualise the true and predicted label.\n",
    "\n",
    "_Note: As of now, the data splits are on the randomised frames, which means that there is a chance for the model to have seen frames that are very similar to the frames in the test set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test split and loader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "test_data = ImageDataset(split=\"test\", include_classes=list(class2id.keys()), ratio=1.0)\n",
    "test_loader = DataLoader(test_data, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a0e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load batch of 16 images\n",
    "test_list = list(iter(test_loader))\n",
    "idx = random.randint(0, len(test_list))\n",
    "images, labels = test_list[idx]\n",
    "\n",
    "# predict\n",
    "logits = model(transform(images))\n",
    "probs = softmax(logits, 1)\n",
    "max_probs, preds = torch.max(probs, 1)\n",
    "\n",
    "# show images with ground truth\n",
    "show_images(images, titles=[f\"True: {id2class[labels[i].item()]}\\nPred: {id2class[preds[i].item()]} ({round(100 * max_probs[i].item(), 1)}%)\" for i in range(len(preds))], show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e269d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on single image\n",
    "image, label = images[0], labels[0]\n",
    "\n",
    "logits = model(transform(image).unsqueeze(0))\n",
    "probs = softmax(logits, 1)\n",
    "prob, pred = torch.max(probs, 1)\n",
    "\n",
    "show_image(image, title=f\"Label: {id2class[label.item()]}\\nPred: {id2class[pred.item()]} ({round(prob.item() * 100,1)}%)\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e686b5",
   "metadata": {},
   "source": [
    "## Evaluation Metrics on Test Split\n",
    "\n",
    "We predict on all samples in the test split and measure common metrics for classification experiments, like accuracy, precision, recall and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all test samples\n",
    "images_mispred = []\n",
    "y_true_mispred, y_pred_mispred = [], []\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        logits = model(transform(images)) # B, C\n",
    "        preds = logits.argmax(-1) # B\n",
    "    \n",
    "        for image, true, pred in zip(images, labels.tolist(), preds.tolist()):\n",
    "            y_true.append(true)\n",
    "            y_pred.append(pred)\n",
    "            if true != pred:\n",
    "                images_mispred.append(image)\n",
    "                y_true_mispred.append(true)\n",
    "                y_pred_mispred.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f4a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labels = list(class2id.keys())\n",
    "report = classification_report(y_true, y_pred, target_names=labels, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391839ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eea26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise confusion matrix with absolute counts\n",
    "_, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', xticklabels=labels, yticklabels=labels, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c1485",
   "metadata": {},
   "source": [
    "## Mispredictions\n",
    "\n",
    "Mispredictions can be informative to investigate how to further improve a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb6b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_true_mispred)\n",
    "print(y_pred_mispred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a70fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 16 random mispredictions\n",
    "idxs = random.sample(range(len(images_mispred)), 16)\n",
    "show_images(\n",
    "    torch.cat([image.unsqueeze(0) for image in images_mispred])[idxs],\n",
    "    titles=[f\"True: {id2class[y_true_mispred[i]]}\\nPred: {id2class[y_pred_mispred[i]]}\" \n",
    "            for i in idxs], show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a575635",
   "metadata": {},
   "source": [
    "We can identify the following typical error sources:\n",
    "\n",
    "- Inherently difficult to predict frames (white wall, close-up of bookshelf)\n",
    "- Similarities of locations (e.g. coloured areas on the different floors)\n",
    "- Transistions between areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedbb2b0",
   "metadata": {},
   "source": [
    "## Predict on Video Clips\n",
    "\n",
    "Real-time inference similar to the final deployed model on mobile devices to get a feel for the consistency of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02257ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# get random video path\n",
    "path = os.path.join(RAW_DATA_PATH)\n",
    "clip = random.sample(os.listdir(path), 1)[0]\n",
    "video_path = os.path.join(path, clip, \"video.mov\")\n",
    "print(f\"Predicting on video {video_path}\")\n",
    "\n",
    "# load video\n",
    "video, _, _ = torchvision.io.read_video(video_path, start_pts=0, end_pts=10, pts_unit=\"sec\", output_format=\"TCHW\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(f\"{MODEL}:{VERSION}\") # type: ignore\n",
    "ax.set_xticks([]) # type: ignore\n",
    "ax.set_yticks([]) # type: ignore\n",
    "\n",
    "img = ax.imshow(transforms.ToPILImage()(video[0])) # type: ignore\n",
    "\n",
    "def animate(i):\n",
    "    # transforms\n",
    "    logits = model(transform(video[i]).unsqueeze(0))\n",
    "    probs = softmax(logits, 1)\n",
    "    prob, pred = torch.max(probs, 1)\n",
    "    prob, pred = prob.item(), pred.item()\n",
    "\n",
    "    print(f\"Prediction: {id2class[pred]} (Confidence: {round(prob * 100, 2)}%)\", end=\"\\r\")\n",
    "\n",
    "    img.set_array(transforms.ToPILImage()(video[i])) # type: ignore\n",
    "\n",
    "    return [img]\n",
    "\n",
    "a = animation.FuncAnimation(fig, animate, frames=len(video), interval=1, blit=True)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(a.to_jshtml())\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18314ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision\n",
    "\n",
    "# get random video path\n",
    "path = os.path.join(RAW_DATA_PATH)\n",
    "clip = random.sample(os.listdir(path), 1)[0]\n",
    "video_path = os.path.join(path, clip, \"video.mov\")\n",
    "print(f\"Predicting on video {video_path}\")\n",
    "\n",
    "video, _, _ = torchvision.io.read_video(video_path, start_pts=0, end_pts=1, pts_unit=\"sec\", output_format=\"TCHW\")\n",
    "# set up video capture\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "while True:\n",
    "    # read next frame\n",
    "    ret, frame = cap.read()\n",
    "    frame_tensor = torch.tensor(frame).permute(2,0,1) # C, H, W\n",
    "    frame_tensor = frame_tensor[[2,1,0], :, :] # change channel to RGB from BGR\n",
    "    \n",
    "    #print(model(transform(video[0].unsqueeze(0))).argmax(-1))\n",
    "    #print(model(transform(frame_tensor[0].unsqueeze(0))).argmax(-1))\n",
    "    \n",
    "    # show_image(transform(video[0]), show=True)\n",
    "    # show_image(transform(frame_tensor[0]), show=True)\n",
    "    \n",
    "    if frame_tensor == None:\n",
    "        break\n",
    "    \n",
    "    # predict frame\n",
    "    logits = model(transform(frame_tensor).unsqueeze(0))\n",
    "    probs = softmax(logits, 1)\n",
    "    prob, pred = torch.max(probs, 1)\n",
    "    prob, pred = prob.item(), pred.item()\n",
    "    class_label = id2class[pred]\n",
    "    \n",
    "    text = f\"{class_label} ({round(100 * prob, 1)}%)\"\n",
    "        \n",
    "    # overlay the prediction on the frame\n",
    "    cv2.putText(frame, text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 1)\n",
    "    \n",
    "    # display the frame with the prediction overlaid\n",
    "    cv2.imshow(f\"{MODEL}:{VERSION}\", frame)\n",
    "    \n",
    "    # exit the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the video capture and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2809649",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = test_data[random.randint(0, len(test_data))]\n",
    "\n",
    "pred = model(transform(image).unsqueeze(0)).argmax(-1)\n",
    "show_image(image, title=id2class[pred.item()], show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ed6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((torch.rand(15,3,224,224), image.unsqueeze(0))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf8bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "preds = model(transform(images)).argmax(-1)\n",
    "show_images(images, titles=[id2class[pred.item()] for pred in preds], show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34458fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__call__(torch.rand(16,3,224,224))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
