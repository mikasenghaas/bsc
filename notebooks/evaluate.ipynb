{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5eb8df",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "This notebook contains a wide variety of methods to evaluate a trained model that was logged to this public [W&B Experiment](https://wandb.ai/mikasenghaas/bsc?workspace=user-mikasenghaas). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f2e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348489e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# custom scripts\n",
    "from config import *\n",
    "from utils import *\n",
    "from model import MODELS, FinetunedImageClassifier\n",
    "from transform import ImageTransformer\n",
    "from data import ImageDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadfefc4",
   "metadata": {},
   "source": [
    "## Load Trained Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e64be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which model to use\n",
    "MODEL = \"resnet18\"\n",
    "VERSION = \"v0\"\n",
    "assert MODEL in MODELS, f\"Specified model has to be one of {list(MODELS.keys())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the artifacts from the wandb server\n",
    "import wandb\n",
    "\n",
    "SAVE_PATH = path = os.path.join(BASEPATH, \"artifacts\", f\"{MODEL}:{VERSION}\")\n",
    "\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact(f'mikasenghaas/bsc/{MODEL}:{VERSION}', type='model')\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    relative_path = artifact.download(root=SAVE_PATH)\n",
    "\n",
    "print(f\"{MODEL}:{VERSION} downloaded to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the paths of the most recently trained model\n",
    "model_path = os.path.join(SAVE_PATH, f\"{MODEL}.pt\")\n",
    "config_path = os.path.join(SAVE_PATH, \"config.json\")\n",
    "transforms_path = os.path.join(SAVE_PATH, \"transforms.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load transform\n",
    "transform = load_pickle(transforms_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "config = load_json(config_path)\n",
    "class2id = config['class2id']\n",
    "id2class = {i:c for c,i in class2id.items()}\n",
    "model = FinetunedImageClassifier(**config)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075bb1ff",
   "metadata": {},
   "source": [
    "## Prediction Examples on Test Split\n",
    "\n",
    "We sample a random batch of `16` frames from the dataset and visualise the true and predicted label.\n",
    "\n",
    "_Note: As of now, the data splits are on the randomised frames, which means that there is a chance for the model to have seen frames that are very similar to the frames in the test set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test split and loader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "test_data = ImageDataset(split=\"test\", include_classes=list(class2id.keys()), ratio=1.0, class2id=class2id, id2class=id2class)\n",
    "test_loader = DataLoader(test_data, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a0e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load batch of 16 images\n",
    "test_list = list(iter(test_loader))\n",
    "idx = random.randint(0, len(test_list))\n",
    "images, labels = test_list[idx]\n",
    "\n",
    "# predict\n",
    "logits = model(transform(images))\n",
    "probs = softmax(logits, 1)\n",
    "max_probs, preds = torch.max(probs, 1)\n",
    "\n",
    "# show images with ground truth\n",
    "show_images(images, titles=[f\"True: {id2class[labels[i].item()]}\\nPred: {id2class[preds[i].item()]} ({round(100 * max_probs[i].item(), 1)}%)\" for i in range(len(preds))], show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on single image\n",
    "image, label = images[0], labels[0]\n",
    "\n",
    "logits = model(transform(image).unsqueeze(0))\n",
    "probs = softmax(logits, 1)\n",
    "prob, pred = torch.max(probs, 1)\n",
    "\n",
    "show_image(image, title=f\"Label: {id2class[label.item()]}\\nPred: {id2class[pred.item()]} ({round(prob.item() * 100,1)}%)\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e686b5",
   "metadata": {},
   "source": [
    "## Evaluation Metrics on Test Split\n",
    "\n",
    "We predict on all samples in the test split and measure common metrics for classification experiments, like accuracy, precision, recall and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all test samples\n",
    "images_mispred = []\n",
    "y_true_mispred, y_pred_mispred = [], []\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        logits = model(transform(images)) # B, C\n",
    "        preds = logits.argmax(-1) # B\n",
    "    \n",
    "        for image, true, pred in zip(images, labels.tolist(), preds.tolist()):\n",
    "            y_true.append(true)\n",
    "            y_pred.append(pred)\n",
    "            if true != pred:\n",
    "                images_mispred.append(image)\n",
    "                y_true_mispred.append(true)\n",
    "                y_pred_mispred.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f4a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labels = list(class2id.keys())\n",
    "report = classification_report(y_true, y_pred, target_names=labels, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391839ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eea26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise confusion matrix with absolute counts\n",
    "_, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', xticklabels=labels, yticklabels=labels, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c1485",
   "metadata": {},
   "source": [
    "## Mispredictions\n",
    "\n",
    "Mispredictions can be informative to investigate how to further improve a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb6b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_true_mispred)\n",
    "print(y_pred_mispred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a70fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 16 random mispredictions\n",
    "idxs = random.sample(range(len(images_mispred)), 16)\n",
    "show_images(\n",
    "    torch.cat([image.unsqueeze(0) for image in images_mispred])[idxs],\n",
    "    titles=[f\"True: {id2class[y_true_mispred[i]]}\\nPred: {id2class[y_pred_mispred[i]]}\" \n",
    "            for i in idxs], show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a575635",
   "metadata": {},
   "source": [
    "We can identify the following typical error sources:\n",
    "\n",
    "- Inherently difficult to predict frames (white wall, close-up of bookshelf)\n",
    "- Similarities of locations (e.g. coloured areas on the different floors)\n",
    "- Transistions between areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41e1b45",
   "metadata": {},
   "source": [
    "## GradCam on Random Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# get random frame from test split\n",
    "target_layers = [model.model.layer4[-1]]\n",
    "random_idx = random.randint(0, len(test_data))\n",
    "image, label = test_data[random_idx]\n",
    "input_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "# construct cam object that observes target layers in model\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False)\n",
    "\n",
    "# pass an input tensor through the cam\n",
    "grayscale_cam = cam(input_tensor) # (1, 224, 224), float32\n",
    "grayscale_cam = grayscale_cam[0, :] # (224, 224), float32\n",
    "\n",
    "# preprocess the raw image to float32\n",
    "normalised_image = image.float() / 255.0 # (3, 224, 224), float32\n",
    "\n",
    "# overlay image with gradcam\n",
    "overlayed_image = show_cam_on_image(np.array(normalised_image.permute(1,2,0)), grayscale_cam, use_rgb=True, image_weight=0.7)\n",
    "\n",
    "# visualise\n",
    "show_image(torch.tensor(overlayed_image).permute(2,0,1), show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedbb2b0",
   "metadata": {},
   "source": [
    "## Predict on Video Clips\n",
    "\n",
    "Real-time inference similar to the final deployed model on mobile devices to get a feel for the consistency of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18314ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision\n",
    "\n",
    "# specify on which split to predict on\n",
    "SPLIT = \"test\"\n",
    "GRADCAM = True\n",
    "\n",
    "# get random video path from split\n",
    "path = os.path.join(RAW_DATA_PATH, SPLIT)\n",
    "clip = random.sample(os.listdir(path), 1)[0]\n",
    "video_path = os.path.join(path, clip, \"video.mov\")\n",
    "print(f\"Predicting on video {video_path}\")\n",
    "\n",
    "# set up video capture\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# construct gram cam\n",
    "if GRADCAM:\n",
    "    target_layers = [model.model.layer4[-1]]\n",
    "    cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False)\n",
    "\n",
    "while True:\n",
    "    # read next frame\n",
    "    ret, frame = cap.read() # np.ndarray, (H=1920, W=1080, C=3), dtype=np.uint8\n",
    "    frame_tensor = torch.tensor(frame).permute(2,0,1) # torch.tensor, (C, H, W)\n",
    "    frame_tensor = frame_tensor[[2,1,0], :, :] # change channel to RGB from BGR\n",
    "    transformed_tensor = transform(frame_tensor).unsqueeze(0) # transform tensor\n",
    "    \n",
    "    if frame_tensor == None:\n",
    "        break\n",
    "    \n",
    "    # predict frame\n",
    "    logits = model(transformed_tensor)\n",
    "    probs = softmax(logits, 1)\n",
    "    prob, pred = torch.max(probs, 1)\n",
    "    prob, pred = prob.item(), pred.item()\n",
    "    class_label = id2class[pred]\n",
    "    \n",
    "    # gradcam heatmap\n",
    "    if GRADCAM:\n",
    "        gradcam = cam(transformed_tensor).squeeze() # np.ndarray, (224, 224), dtype=np.uint8\n",
    "        gradcam = cv2.resize(gradcam, (frame.shape[1], frame.shape[0]))\n",
    "        \n",
    "        normalised_frame = frame.astype(np.float32) / 255.0 # np.ndarray, (1920, 1080, 3), dtype=np.float32\n",
    "        \n",
    "        frame = show_cam_on_image(normalised_frame, gradcam, image_weight=0.7)\n",
    "    \n",
    "    text = f\"{class_label} ({round(100 * prob, 1)}%)\"\n",
    "        \n",
    "    # overlay t he prediction on the frame\n",
    "    cv2.putText(frame, text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 1)\n",
    "    \n",
    "    # display the frame with the prediction overlaid\n",
    "    cv2.imshow(f\"{MODEL}:{VERSION}\", frame)\n",
    "    \n",
    "    # exit the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the video capture and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
