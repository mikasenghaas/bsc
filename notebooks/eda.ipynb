{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B7h2F2QdCVsC"
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFN9JCiSCX3y"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from defaults import DEFAULT\n",
    "from config import RAW_DATA_PATH\n",
    "from utils import ls, show_images\n",
    "from data import ImageDataset, VideoDataset\n",
    "from transform import ImageTransform, VideoTransform\n",
    "from model import ImageClassifier, VideoClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_meta = []\n",
    "for split in [\"train\", \"test\"]:\n",
    "    for clip in sorted(ls(os.path.join(RAW_DATA_PATH, split))):\n",
    "        datestr, num = clip.split('_')\n",
    "        date = datetime.strptime(datestr, \"%y%m%d\")\n",
    "        \n",
    "        filepath = os.path.join(RAW_DATA_PATH, split, clip, \"video.mov\")\n",
    "        cap = cv2.VideoCapture(filepath)\n",
    "        \n",
    "        # compute duration\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = int(frame_count / fps)\n",
    "        \n",
    "        # save information\n",
    "        raw_meta.append({\n",
    "            \"split\": split,\n",
    "            \"clip\": clip,\n",
    "            \"date\": date,\n",
    "            \"seconds\": duration,\n",
    "            \"frames\": frame_count / 1000\n",
    "        })\n",
    "        \n",
    "raw_meta = pd.DataFrame(raw_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seconds by day\n",
    "raw_meta.groupby(\"date\").sum(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seconds by split\n",
    "raw_meta.groupby(\"split\").sum(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seconds by split and date\n",
    "raw_meta.groupby([\"split\", \"date\"]).sum(\"seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image dataset\n",
    "train_images = ImageDataset(split=\"train\", transform=None)\n",
    "test_images = ImageDataset(split=\"test\", transform=None)\n",
    "\n",
    "num_train_images = len(train_images)\n",
    "num_test_images = len(test_images)\n",
    "\n",
    "print(f\"Number of training images: {num_train_images}\")\n",
    "print(f\"Number of test images: {num_test_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image dataset\n",
    "frame_rate = 8\n",
    "num_frames = 8\n",
    "clip_duration = num_frames * frame_rate / 30\n",
    "train_clips = VideoDataset(split=\"train\", clip_duration=clip_duration, transform=None)\n",
    "test_clips = VideoDataset(split=\"test\", clip_duration=clip_duration, transform=None)\n",
    "\n",
    "num_train_clips = 0\n",
    "num_test_clips = 0\n",
    "for clip in train_clips:\n",
    "    num_train_clips += 1\n",
    "\n",
    "for clip in test_clips:\n",
    "    num_test_clips += 1\n",
    "\n",
    "print(f\"Number of training clips (Clip Duration: 2.1s): {num_train_clips}\")\n",
    "print(f\"Number of test clips (Clip Duration: 2.1s): {num_test_clips}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image dataset\n",
    "clip_duration = 1\n",
    "train_clips = VideoDataset(split=\"train\", clip_duration=clip_duration, transform=None)\n",
    "test_clips = VideoDataset(split=\"test\", clip_duration=clip_duration, transform=None)\n",
    "\n",
    "num_train_clips = 0\n",
    "num_test_clips = 0\n",
    "for clip in train_clips:\n",
    "    num_train_clips += 1\n",
    "\n",
    "for clip in test_clips:\n",
    "    num_test_clips += 1\n",
    "\n",
    "print(f\"Number of training clips (Clip Duration: 1s): {num_train_clips}\")\n",
    "print(f\"Number of test clips (Clip Duration: 1s): {num_test_clips}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image dataset\n",
    "clip_duration = 2\n",
    "train_clips = VideoDataset(split=\"train\", clip_duration=clip_duration, transform=None)\n",
    "test_clips = VideoDataset(split=\"test\", clip_duration=clip_duration, transform=None)\n",
    "\n",
    "num_train_clips = 0\n",
    "num_test_clips = 0\n",
    "for clip in train_clips:\n",
    "    num_train_clips += 1\n",
    "\n",
    "for clip in test_clips:\n",
    "    num_test_clips += 1\n",
    "\n",
    "print(f\"Number of training clips (Clip Duration: 2s): {num_train_clips}\")\n",
    "print(f\"Number of test clips (Clip Duration: 2s): {num_test_clips}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calendar plot for timing\n",
    "import calplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "    \n",
    "dates = pd.Series(raw_meta.groupby(\"date\").sum(\"seconds\"))\n",
    "fig, ax = calplot.calplot(dates, cmap='YlGn', colorbar=False);\n",
    "\n",
    "# fig.savefig(\"../report/figures/data-collection-freq.jpg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ImageDataset` class is used for all image classification modules. After pre-processing, the frames are stored in the following way\n",
    "\n",
    "```\n",
    "filepath\n",
    "|-class1\n",
    "|  |__ frame1.jpg\n",
    "|  |__ frame2.jpg\n",
    "|  |__ ...\n",
    "|_class2\n",
    "|  |__ frame1.jpg\n",
    "|  |__ frame2.jpg\n",
    "|  |__ ...\n",
    "```\n",
    "\n",
    "With this, all frames can easily be read into a `torchvision.datasets.ImageFolder` class, which takes in `filepath` and a custom `transforms` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise data\n",
    "config = DEFAULT[\"resnet18\"]\n",
    "transform = ImageTransform(**config[\"transform\"])\n",
    "train_image_data = ImageDataset(split=\"train\", transform=transform)\n",
    "test_image_data = ImageDataset(split=\"test\", transform=transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ImageDataset` instances `train_image_data` and `test_image_data` provide handy ways to load unbatched samples by implementing the `iter` and `getitem` dunder methods. Let's see how many frames there are in either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of frames\n",
    "print(len(train_image_data))\n",
    "print(len(test_image_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the test split way larger? That is because in the `preprocess` script, frames for the train and test split are extracted. In there, it is specified that for only 1 frame per second (FPS) should be sampled for the training split, because it is hypothesised that including too many too similiar training frames will make the model overfit heavily. For the test split, however, this cannot be said. We would like to evaluate the model on all frames at the normal frame rate of 30FPS to get as robust as possible test metrics.\n",
    "\n",
    "Let's see if we can say something about the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num classes\n",
    "print(train_image_data.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_image_data.id2class, index=[\"class\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class distribution for train class\n",
    "train_classes = [cls for (_, cls) in train_image_data.dataset.imgs]\n",
    "train_class_dist = pd.DataFrame(train_classes, columns=[\"class\"]).value_counts().to_frame().reset_index()\n",
    "train_class_dist.columns = [\"class\", \"count\"]\n",
    "\n",
    "test_classes = [cls for (_, cls) in test_image_data.dataset.imgs]\n",
    "test_class_dist = pd.DataFrame(test_classes, columns=[\"class\"]).value_counts().to_frame().reset_index()\n",
    "test_class_dist.columns = [\"class\", \"count\"]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "sns.barplot(data=train_class_dist, x=\"class\", y=\"count\", palette=\"Dark2\", ax=axs[0])\n",
    "sns.barplot(data=test_class_dist, x=\"class\", y=\"count\", palette=\"Dark2\", ax=axs[1])\n",
    "axs[0].set_xticklabels(list(train_image_data.id2class.values()), rotation=90);\n",
    "axs[1].set_xticklabels(list(test_image_data.id2class.values()), rotation=90);\n",
    "\n",
    "# rotate x tick labels 90 degrees\n",
    "for ax in axs:\n",
    "  ax.set_xlabel(\"Class\")\n",
    "  ax.set_ylabel(\"Count\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks about right. The testing distribution is not quite the same as in the training data, but that is fine. After all, we don't expect people to always take the same route through the building. With this established, let's look at an example batch, as used in tthe training loop of image classifiers.\n",
    "\n",
    "To achieve that, let's first define a data loader (with the configurations for ResNet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "train_image_loader = DataLoader(train_image_data, **config[\"loader\"])\n",
    "test_image_loader = DataLoader(test_image_data, **config[\"loader\"])\n",
    "\n",
    "print(f\"Batch Size: {config['loader']['batch_size']}\")\n",
    "print(f\"Training Batches: {len(train_image_loader)}\")\n",
    "print(f\"Testing Batchs: {len(test_image_loader)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with a batch size of 32, there are 70 training batches and 734 testing batches. In the training loop, we want to have some knowledge about the out-of-sample performance. Because we don't do any hyperparameter tuning, we can use the test split as a validation split. However, we don't want to test on ~22k frames after each epoch. For this reason we are using the `torch.utils.Subset` class and sample 5% of the entire testing data as an approximation of the test split, which we call validation split. Let's see how that would look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = int(len(test_image_data) * 0.05)\n",
    "indices = np.random.choice(len(test_image_data), k)\n",
    "val_image_data = Subset(train_image_data, indices)\n",
    "val_image_loader = DataLoader(val_image_data, **config[\"loader\"])\n",
    "\n",
    "print(f\"Validation Samples: {len(val_image_data)}\")\n",
    "print(f\"Validation Batches: {len(val_image_loader)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 5% of the test split, randomly sampled, for validation. Nice! Let's look at an example batch now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, labels = next(iter(train_image_loader))\n",
    "frames, labels = frames[:9], labels[:9]\n",
    "show_images(frames, titles=[train_image_data.id2class[label.item()] for label in labels], unnormalise=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks perfect! We see that the training batch is shuffled, the annotation and class-id mapping seems to work, and transforms to the images work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VideoDataset\n",
    "\n",
    "Let's now turn to the `VideoDataset` class. It is conceptually similar, as it is just a wrapper around the `LabeledVideoDataset` class provided by `pytorchvideo`. It is an instance of a PyTorch `IterableDataset` and therefore does not compute the number of samples in `len` method.\n",
    "\n",
    "To showcase the video dataset class, we will use the configuration specification of `R2+1D(18)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DEFAULT[\"r2plus1d_18\"]\n",
    "\n",
    "video_transform = VideoTransform(**config[\"transform\"])\n",
    "train_video_data = VideoDataset(**config[\"dataset\"], split=\"train\", transform=video_transform)\n",
    "test_video_data = VideoDataset(**config[\"dataset\"], split=\"test\", transform=video_transform, sampler=\"sequential\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot get the number of videos/ clips from this instance, but we can iterate over it. Let's do that to count the number of training and testing clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_clips = 0\n",
    "num_test_clips = 0\n",
    "for i, _ in enumerate(train_video_data):\n",
    "    num_train_clips += 1\n",
    "for i, _ in enumerate(test_video_data):\n",
    "    num_test_clips += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Clip Duration: {config['dataset']['clip_duration']}s\")\n",
    "print(f\"Training Clips: {num_train_clips} ({num_train_clips * config['dataset']['clip_duration']}s)\")\n",
    "print(f\"Testing Clips: {num_test_clips} ({num_test_clips * config['dataset']['clip_duration']}s)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, but it looks like we are almost loosing 50% of the data, because most video clips have been preprocessed to a max length of 5s, which means that the remaining 2.4s of the video clip are not used. \n",
    "\n",
    "Let's look at how a single sample can be obtained from the training video dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_video_data))\n",
    "print(sample.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the labelled video dataset class returns a sample as dictionary, with the following keys:\n",
    "\n",
    "- `video`: `torch.Tensor` of dim (C,T,H,W), the actual clip\n",
    "- `video_name`: `str`, name of video\n",
    "- `video_index`: `int`\n",
    "- `clip_index`: `int`\n",
    "- `label`: `str`, class of clip\n",
    "\n",
    "This means that before passing it into a model, we will have to 1) extract the video tensor and 2) encode the string representation of the label.\n",
    "\n",
    "Another important thing is the differentiation between a video and a clip. Video classification models cannot handle arbitrary length videos (dim T in tensor), but have some fixed capacity. On top of that, they define the parameter `sample_rate`, which specifies the rate at which frames are taken into account from the original 30FPS stream of frames.\n",
    "\n",
    "In the example of `x3d_s` the frame rate is 6 and the number of frame is 13. This means that for a sequence of frames `x`, only the `o` are used in the model. Because the model assumes to get `13` frames, one training/ inference clip has to consist of at least 6*13=78 frames. In a 30FPS video, this means around ~2.6s. The VideoDataset class handles all that itself, but this is the reason, why there is a clip index.\n",
    "\n",
    "```\n",
    "oxxxxxoxxxxxoxxxxxo...\n",
    "```\n",
    "\n",
    "If one sets `sampler=\"random\"` then the video clips are sampled randomly. One video always starts at the first clip, and if it gets sampled again goes to the next clip, if there is enough seconds left. \n",
    "\n",
    "If one sets `sampler=\"sequential\"` then the video clips are sampled sequentially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def display_video(video, label, config):\n",
    "  mean = np.array(config[\"transform\"][\"mean\"])\n",
    "  std = np.array(config[\"transform\"][\"std\"])\n",
    "  video = video.permute(1,0,2,3)\n",
    "  video_widget = widgets.Image(format='jpeg')\n",
    "\n",
    "  # display the widget\n",
    "  display(video_widget)\n",
    "  for frame in video:\n",
    "    img = plt.imshow(np.array(((frame * std[:, None, None] + mean[:, None, None]) * 255.0).permute(1,2,0), dtype=np.uint8))\n",
    "    plt.title(label)\n",
    "    buffer = io.BytesIO()\n",
    "    plt.savefig(buffer, format='jpeg')\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    video_widget.value = buffer.getvalue()\n",
    "    time.sleep(1)\n",
    "\n",
    "display_video(sample[\"video\"], sample[\"label\"], config) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks, nice. Let's define a loader class to observe the differnet video/ clip sampling behavior for the train and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_video_loader = DataLoader(train_video_data, **config[\"loader\"])\n",
    "test_video_loader = DataLoader(test_video_data, **config[\"loader\"])\n",
    "\n",
    "print(f\"Batch Size: {config['loader']['batch_size']}\")\n",
    "print(\"\\nTraining (Random Video Sampling)\")\n",
    "train_batch = next(iter(train_video_loader))\n",
    "print(f\"Batch Tensor Shape: {train_batch['video'].shape}\")\n",
    "print(f\"Video Names: {train_batch['video_name']}\")\n",
    "print(f\"Clip Index: {train_batch['clip_index']}\")\n",
    "print(f\"Labels: {train_batch['label']}\")\n",
    "\n",
    "print(\"\\nTesting (Sequential Video Sampling)\")\n",
    "test_batch = next(iter(test_video_loader))\n",
    "print(f\"Batch Tensor Shape: {test_batch['video'].shape}\")\n",
    "print(f\"Video Names: {test_batch['video_name']}\")\n",
    "print(f\"Clip Index: {test_batch['clip_index']}\")\n",
    "print(f\"Labels: {test_batch['label']}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
