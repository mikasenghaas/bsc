{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7h2F2QdCVsC"
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFN9JCiSCX3y"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from utils import ls\n",
    "from data import ImageDataset, VideoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "def render_latex(df):\n",
    "    # capitalise col names\n",
    "    df.columns = [' '.join(map(lambda x: x[0].upper() + x[1:], col.split('_'))) for col in df.columns]\n",
    "    \n",
    "    # format df\n",
    "    s = df.style.highlight_max(props='bfseries: ;')\n",
    "    s.format(precision=2)\n",
    "    \n",
    "    # render latex\n",
    "    opts = {\"hrules\": True, \"position\": \"h\"}\n",
    "    return s.to_latex(**opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_meta = []\n",
    "for split in SPLITS:\n",
    "    for clip in sorted(ls(os.path.join(RAW_DATA_PATH, split))):\n",
    "        datestr, num = clip.split('_')\n",
    "        date = datetime.strptime(datestr, \"%y%m%d\")\n",
    "        \n",
    "        filepath = os.path.join(RAW_DATA_PATH, split, clip, \"video.mov\")\n",
    "        cap = cv2.VideoCapture(filepath)\n",
    "        \n",
    "        # compute duration\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = int(frame_count / fps)\n",
    "        \n",
    "        # save information\n",
    "        raw_meta.append({\n",
    "            \"split\": split,\n",
    "            \"clip\": clip,\n",
    "            \"date\": date,\n",
    "            \"seconds\": duration\n",
    "        })\n",
    "        \n",
    "raw_meta = pd.DataFrame(raw_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of seconds in two splits\n",
    "raw_meta.groupby(\"split\").sum(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise data splits\n",
    "image_data = { split: ImageDataset(split=split, include_classes=CLASSES, ratio=1.0) for split in SPLITS }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise data loaders\n",
    "image_loader = { split: DataLoader(image_data[split], 9) for split in SPLITS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics about Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some statistics about the data\n",
    "statistics = []\n",
    "for split in SPLITS:\n",
    "    clips = image_data[split].frames_by_clip.keys()\n",
    "    \n",
    "    num_clips = len(clips)\n",
    "    num_frames = sum([len(image_data[split].frames_by_clip[clip]) for clip in clips])\n",
    "    total_seconds = int(num_frames / FPS)\n",
    "    total_mins = round(total_seconds / 60)\n",
    "    \n",
    "    statistics.append({\n",
    "        \"split\": split,\n",
    "        \"num_clips\": num_clips,\n",
    "        \"total_seconds\": total_seconds,\n",
    "        \"total_mins\": total_mins\n",
    "    })\n",
    "    \n",
    "stats = pd.DataFrame(statistics).set_index(\"split\")\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(render_latex(stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calendar plot for timing\n",
    "import calplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "clips_per_day = defaultdict(int)\n",
    "for split in SPLITS:\n",
    "    clips = image_data[split].frames_by_clip.keys()\n",
    "    for clip in clips:\n",
    "        datestr, num = clip.split('_')\n",
    "        date = datetime.strptime(datestr, '%y%m%d')\n",
    "        clips_per_day[date] += 1\n",
    "    \n",
    "dates = pd.Series(clips_per_day)\n",
    "fig, ax = calplot.calplot(dates, cmap='YlGn', colorbar=False);\n",
    "\n",
    "fig.savefig(\"../report/figures/data-collection-freq.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calendar plot for timing\n",
    "import calplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "    \n",
    "for key in clips_per_day.keys():\n",
    "    clips_per_day[key] = 5\n",
    "# training days\n",
    "clips_per_day[datetime.strptime('230222', '%y%m%d')] = 10\n",
    "clips_per_day[datetime.strptime('230302', '%y%m%d')] = 10\n",
    "\n",
    "dates = pd.Series(clips_per_day)\n",
    "calplot.calplot(dates, cmap='cool', colorbar=False);\n",
    "\n",
    "fig.savefig(\"../report/figures/data-collection-splits.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validitity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify disjointness of splits\n",
    "clips = {}\n",
    "for split in SPLITS:\n",
    "    clips[split] = set([clip for clip in image_data[split].frames_by_clip.keys()])\n",
    "\n",
    "for pair in combinations(SPLITS, 2):\n",
    "    fst, snd = pair\n",
    "    print(f\"{pair} has {len(clips[fst] & clips[snd])} images in common\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify even class distribution\n",
    "fig, ax = plt.subplots(ncols=len(SPLITS), figsize=(4*len(SPLITS),3))\n",
    "for i, split in enumerate(SPLITS):\n",
    "    dist = image_data[split].class_distribution\n",
    "    sns.barplot(x=list(dist.keys()), y=list(dist.values()), ax=ax[i])\n",
    "    ax[i].tick_params(axis='x', rotation=90)\n",
    "    ax[i].set(title=f\"{split.capitalize()} Split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Training Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get example preprocessed clips\n",
    "frames = list(iter(image_data[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 30\n",
    "n_frames = 4\n",
    "images, labels = zip(*frames[start_idx:start_idx+n_frames])\n",
    "images = torch.cat([i.unsqueeze(0) for i in images])\n",
    "\n",
    "fig, axs = plt.subplots(ncols=n_frames, figsize=(3*n_frames, 3))\n",
    "for i in range(n_frames):\n",
    "    show_image(images[i], title=image_data[\"train\"].id2class[labels[i]], ax=axs[i])\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"../report/figures/data-example-batch.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train split\n",
    "images, labels = next(iter(image_loader[\"train\"]))\n",
    "\n",
    "show_images(images, titles=[image_data[\"train\"].id2class[l.item()] for l in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test split\n",
    "images, labels = next(iter(image_loader[\"test\"]))\n",
    "\n",
    "show_images(images, titles=[image_data[\"test\"].id2class[l.item()] for l in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = VideoDataset.default_config()\n",
    "video_dataset = VideoDataset(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "\n",
    "clip, labels = video_dataset[random.randint(0, len(video_dataset))]\n",
    "image_tensors = [image for image in clip]\n",
    "\n",
    "# create a figure and axis object\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# create an empty image object to hold the current frame\n",
    "im = ax.imshow(np.zeros_like(image_tensors[0].permute(1,2,0)))\n",
    "\n",
    "# define the update function that will be called for each frame\n",
    "def update(i):\n",
    "    im.set_data(image_tensors[i].permute(1, 2, 0))\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=len(image_tensors), interval=500, blit=True)\n",
    "\n",
    "ani.save('animation.mp4', writer='ffmpeg')\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(f'<video controls src=\"animation.mp4\" />')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of clips\n",
    "len(video_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only clips with 10 frames in length\n",
    "np.unique([len(clip) for clip, _ in video_dataset], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
