%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Define Article %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Using Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{color}
\usepackage{nth}
\usepackage{bm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{array}
\usepackage{multirow}
\usepackage{parskip}
%%%%%%%%%%%%%%%%%%%%%%%%%% Page Setting %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\geometry{a4paper}

% sans-serif font
\renewcommand{\familydefault}{\sfdefault}

%%%%%%%%%%%%%%%%%%%%%%%%%% Define some useful colors %%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{ocre}{RGB}{243,102,25}
\definecolor{mygray}{RGB}{243,243,244}
\definecolor{deepGreen}{RGB}{26,111,0}
\definecolor{shallowGreen}{RGB}{235,255,255}
\definecolor{deepBlue}{RGB}{61,124,222}
\definecolor{shallowBlue}{RGB}{235,249,255}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%% Macros %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator{\Lagr}{\mathcal{L}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
  % custom title page
  \include{title.tex}
  \newpage

  % table of contents, list of tables, list of figures
  \tableofcontents
  % \listoftables
  % \listoffigures
  \newpage

  % Deep residual networks have emerged as a family of extremely deep
  % architectures showing compelling accuracy and nice convergence behaviors. In
  % this paper, we analyze the propagation formulations behind the residual
  % building blocks, which suggest that the forward and backward signals can be
  % directly propagated from one block to any other block, when using identity
  % mappings as the skip connections and after-addition activation. A series of
  % ablation experiments support the importance of these identity mappings. This
  % motivates us to propose a new residual unit, which makes training easier and
  % improves generalization. We report improved results using a 1001-layer ResNet
  % on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.
  % Code is available at: https://github.com/KaimingHe/resnet-1k-layers

  \begin{abstract} % (fold)

    Current indoor localisation techniques require rigorous fine-tuning to the
    task at hand, high maintenance efforts and donâ€™t scale well, making
    commercial applications rare. In this context, and in the light of the
    success of deep learning in numerous computer vision tasks, this study
    proposes to phrase indoor localisation as a classification task in
    applications which do not require centimetre-level accuracy. Various
    traditional and state-of-the-art deep learning models, including 2D and 3D
    convolutional, residual, densely connected and self-attention based models,
    are trained and evaluated on a self-collected indoor localisation dataset.
    We show that best performing models show capabilities for generalisation and
    robustness to varying input in a low-resource data setting - giving hope
    for this approach to become a viable alternative to existing techniques.

  \end{abstract}

  \section{Introduction}
  \label{sec:introduction}

  % outdoor localisation as solved problem 
  With the introduction of GPS (Global Positioning System), a satellite-based
  positioning system, localisation in outdoor spaces has become more efficient
  and accurate than ever before. Gradual commercialisation led to the technology
  rapidly transforming industries and personal navigation. Today, outdoor
  localisation is widely considered a \textit{solved problem}.

  % gps struggles in indoor spaces
  The same cannot be said for indoor localisation. Because the transmitted radio
  signals sent out by the satellites in GPS systems are not strong enough to
  penetrate through walls and struggle with reflections from large buildings,
  the technology yields inaccurate results at best, and often becomes
  dysfunctional in indoor spaces~\textit{[CITE]}.

  % indoor localisation techniques (broad overview)
  Finding alternative solutions to provide an accurate, cheap and robust indoor
  localisation systems has been a main focus of research in the past decades,
  and is becoming increasingly important in the light of the ongoing
  urbanisation of our living spaces and the emergence of autonomous robots and
  vehicles in our everyday life. Decades of research have led to the development
  of a variety of different indoor localisation technologies. Hardware-based
  systems use radio signals, transmitted by beacons, like
  Bluetooth~\cite{bluetooth1, bluetooth2}, and Ultra-Wideband (UWB)~\cite{uwb1,
  uwb2} or Wi-Fi~\cite{survey1, survey2, wireless-positioning}, to localise an
  agent in a known environment. Software-based systems, like Simultaneous
  Localisation and Mapping (SLAM)~\cite{mono-slam, ptam, orb-slam} algorithms,
  use sensory information, like cameras or distance-measuring laser sensors, as
  input to a complex pipeline involving various computer vision algorithms to
  ultimately localise an agent in an unknown environment, while also building a
  map of the environment.
  
  % short-comings of existing approaches
  While these approaches have proven to produce remarkable results, being
  capable of localising an agent with centimetre accuracy, they are limited for
  various reasons: Hardware-based system require an expensive initial setup,
  continuous maintenance of the signal-transmitting beacons, and are often not
  feasible in large environments~\textit{[CITE]}. SLAM algorithms, on the other
  hand, require a meticulously handcrafted pipeline of feature detection,
  feature matching, and pose estimation that has to be fine-tuned by experts for
  each indoor space, to achieve outstanding results~\textit{[CITE]}. All of the
  above short-comings of existing indoor localisation technologies make
  commercial applications rare and not unified in their approach. 

  % short-comings of existing approaches
  All of the above approaches work under the assumption that centimetre-accuracy
  is required for indoor localisation. However, this is not always the case. For
  some applications, like indoor navigation, it is sufficient to know the
  position of the agent with a meter or even room-accuracy, instead of
  centimetre accuracy. One example of such an application is the navigation in
  large indoor spaces, like airports, train stations, or shopping malls, where
  the goal is to guide the agent to a specific room or area. In these cases, the
  constraint of centimetre-accuracy can be relaxed, in favour of a simpler and
  more versatile solution.

  % success of deep learning in computer vision
  In the past decades, deep learning has proven to be a powerful tool in a
  wide-variety of tasks and has repeatedly proven remarkable capabilities in the
  field of computer vision. Amongst common tasks in computer vision are the tasks
  of image and video classification, where the goal is to predict a label from a
  set of pre-defined labels for a given image or video.

  % deep learning for indoor localisation
  Motivated by the apparent lack of a simple, unified indoor localisation system
  and the success of deep learning in computer vision, this study investigates
  the applicability of modern deep learning techniques to the task of indoor
  localisation when viewing localisation as a classification task. The study
  presents the rigorous evaluation of several modern deep learning architectures
  on a challengingly small video dataset for mapping out a novel indoor space.

  % section introduction (end)

  \section{Background} % (fold)
  \label{sec:background}

  Creating accurate, robust and cheap indoor localisation systems is not a novel
  task, but has been a focus of research at the intersection of robotics,
  computer vision and machine learning for decades.

  \subsection{Hardware-Based Indoor Localisation} % (fold)
  \label{sub:hardware-based-indoor-localisation}

  The approach conceptually closest to GPS-based outdoor localisation systems
  are hardware-based indoor localisation systems. Because radio signals
  transmitted by satellites are incapable of penetrating through walls, various
  close-proximity radio signals have been proposed to be used for indoor
  localisation, such as Bluetooth~\cite{bluetooth1, bluetooth2}, Ultra-Wideband
  (UWB)~\cite{uwb1, uwb2} or Wi-Fi~\cite{survey1, survey2}.

  No matter the radio signals, three main approaches can be distinguished:
  (1) Angle of Arrival (AOA), (2) Time of Arrival (TOA) and (3)
  RSSI-Fingerprinting (RSSI-FP)~\cite{survey2}. 

  In AOA approaches, transmitting beacons, called access points (APs), are
  measuring the distance and angle between a beacon and an agent. The
  intersection between the lines of sight (LoS) of at least three APs yields
  the position of the agent. 

  TOA approaches (such as GPS) use the received signal strength (RSS) to
  estimate the distance between an agent and the AP through a propagation model.
  Given the distance estimates of at least four APs, the agent's position can be
  determined by an approach called multilateration, which is based on the idea
  that an agent's position in three-dimensional space can be uniquely determined
  by the intersection of four spheres with known radius and centres. Clearly,
  TOA approaches require knowledge about the position of the APs, which makes
  them less practical in indoor spaces, where the position of the APs is often
  unknown.

  The approach with the most recent research interest is RSSI-FP. It can be
  divided into two separate phases: In an offline mapping phase, the indoor
  environment is mapped by repeatedly measuring the received signal strength
  indicator (RSSI) values of the APs at various reference points (RPs). A vector
  of RSSI values then uniquely identifies each RP and is stored in a database.
  In the online localisation phase, the agent's position is then determined by
  dynamically comparing the agent's RSSI vector against the RSSI vectors of the
  RPs stored in the database. Ultimately, the agent's position is determined as
  the position of the RP with the most similar RSSI vector according to some
  similarity metric. While this approach alleviates the need for the precise
  positions of the APs, it requires a time-intensive offline mapping phase and
  is sensitive to changes in the environment.

  Many of the proposed technologies assume massive infrastructure deployments
  and incur high setup and maintenance costs. Wi-Fi based approaches seem most
  promising, given their ubiquitous availability in modern indoor spaces.
  However, all hardware-based approaches suffer measurements errors that
  underlie the triangulation (AOA), multilateration (TOA) or fingerprinting
  (RSSI-FP) approaches. Such errors are caused by reflections (multipath
  effect), blockage (shadowing), and signal attenuation (fading).

  % subsection hardware_based_indoor_localisation (end)k

  \subsection{Simultaneous Location and Mapping (SLAM)} % (fold)

  % SLAM
  Amongst the most promising approaches are SLAM (Simultaneous Localisation and
  Mapping) algorithms. SLAM algorithms aim to localise an agent inside an
  unknown environment, while simultaneously building a consistent map of the
  environment. There exist a variety of different approaches to SLAM, depending
  on the type of sensors that are used. For example, Visual SLAM (V-SLAM)
  algorithms use camera input, and LidarSLAM algorithms use distance-measuring
  laser sensors. 

  Most related to this study are monocular V-SLAM algorithms, because they use a
  single camera to estimate the position of the agent. The very first monocular
  feature-based V-SLAM algorithms, MonoSLAM, was proposed in
  2007~\cite{mono-slam}. The research is considered a break-through in V-SLAM
  algorithms, as it is considered the first algorithm producing accurate results
  while only using a single camera. Previously, V-SLAM algorithms required
  multiple cameras or other sensors to overcome the problem of depth estimation
  using a single camera.

  Since then, many adjustments and optimisation have been proposed to the
  algorithm to make it more robust and accurate. Typically, the adjustments
  replace or modify one of the components of the pipeline. For example, the
  ORB-SLAM~\cite{orb-slam} algorithm uses a bag-of-words approach for feature
  matching, and the PTAM~\cite{ptam} parallelises the computation for
  positioning and map creation, which was shown to improve the accuracy of the
  algorithm. In recent years, modifications are often based on deep learning
  techniques to improve parts of traditional SLAM pipelines. For example, the
  DeepVO~\cite{deep-vo} algorithm uses a convolutional neural network to
  estimate the camera pose from a sequence of images. 

  Overall, SLAM algorithms are promising for indoor localisation due to their
  precision and simultaneous map creation. This makes them state-of-the-art for
  autonomous robots, which are required to localise themselves in unknown
  environments. However, SLAM algorithms are computationally expensive and
  require a lot of memory. This makes them less suitable for mobile devices,
  which are often resource-constrained. Furthermore, to achieve outstanding
  results, the SLAM pipeline needs to be fine-tuned to the specific environment
  and use-case, which can only be done by experts. This makes SLAM algorithms
  less suitable for general-purpose indoor localisation.


  \subsection{Deep Learning in Computer Vision} % (fold)

  % cv as subfield of ai
  Computer vision is one of the major subfields of artificial intelligence and
  is concerned with the automatic extraction of information from images. The
  extraction of information from images is not straight-forward, because images
  are high-dimensional and unstructured. This makes it a challenging environment
  for hand-crafted algorithms, which are often based on heuristics and
  assumptions about the data. For this reason, deep learning techniques have
  been applied successfully to many computer vision tasks, and dominate the
  benchmarks of most computer vision tasks.

  The most dominant architectural paradigm in computer vision is the
  convolutional neural network (CNN). CNNs are a type of neural network, which
  are inspired by the visual cortex, which is responsible for processing visual
  information. Like the visual cortex, CNNs are organised hierarchically and
  consist of a stack of convolutional layers. In each layer, a convolutional
  filter is applied to the input, which is then passed to the next layer. For
  image data, the convolutional filter is a three-dimensional matrix of weights
  that is moving alongside two dimensions (width and height) of the input, and
  is therefore referred to as 2d-convolution. For video data, the convolutional
  filter is a four-dimensional matrix of weights that is moving alongside three
  dimensions (width, height, and time) of the input, and is therefore referred
  to as 3d-convolution. The output of a convolutional layer for each region is
  computed as the dot product of the filter and the input region. 

  % TODO: parameterisation with stride, padding

  While the theory behind CNNs was already established in the
  1980s~\cite{lenet}, practical applications were limited by the lack of
  computational power and large datasets. The offset of the deep learning wave
  in computer vision was the success of the AlexNet~\cite{alexnet} architecture
  in the 2012 ImageNet Large Scale Visual Recognition Challenge~\cite{imagenet},
  which crushed the competition by increasing the Top-5 Accuracy from 73.8\% to
  84.7\%. At the time, the dominating approach to computer vision tasks was
  based on hand-crafted features, such as SIFT~\cite{sift} and HOG~\cite{hog}.
  AlexNet was the first successful adaption of a deep convolutional neural
  network (CNN), which were first proposed by LeCun et al.~\cite{lenet} in 1998. 

  In the following years, CNN-based architectures with increasing depth and
  width, and architectural improvements, such as weight initialisation, batch
  normalisation, residual connections~\cite{vgg, googlenet, resnet, xception}
  were proposed and pushed the state-of-the-art further. The introduction of
  ResNet in 2015 is considered a cornerstone in convolutional neural networks
  architectures, as it allowed for the training of much deeper networks without
  the problem of vanishing gradients. With the success of Transformer-based
  architectures for language modelling in natural language
  processing~\cite{transformer}, the computer vision community was quick to
  adapt the architecture to computer vision tasks. The first successful
  application of the Transformer architecture to computer vision was the Vision
  Transformer (ViT)~\cite{vit}, which achieved state-of-the-art on many image
  classification benchmarks~\cite{imagenet}.

  Beyond image classification, the task of video classification has recently
  gained traction with the introduction of large-scale video datasets, such as
  Kinetics~\cite{kinetics}, more computational resources and an increasing need
  to understand video data in a multi-media world. Continuously predicting on a
  stream of frames is related to image classification, but adds complexity to
  the task through the temporal dimension and increase in data size. Video
  classification can be solved by combining CNNs with recurrent neural networks
  (RNNs) or by using 3D convolutions. The first approach by Donahue et. al
  proposes long-term recurrent convolutional networks (LRCNs), which use a CNN
  to extract features from each frame, which are then fed into a
  LSTM~\cite{lrcn} to produce a sequence of predictions. Alternative approaches
  use 3D convolutions to capture the spatio-temporal nature of video
  data~\cite{c3d, i3d}. Yet another approach, called SlowFast~\cite{slowfast}
  uses two separate CNN networks to capture spatial and temporal information
  separately, which are then combined in the final layers of the network.

  Overall, deep learning has proven to be a powerful tool for computer vision
  that has pushed many state-of-the-art benchmarks to new heights. Already
  today, computer vision algorithms based on deep learning are used in many
  applications, such as autonomous driving, medical imaging and robotics.

  % TODO: explain convolution in 2d and 3d
  % TODO: pooling
  % TODO: dropout, batch norm, weight init

  % subsection deep-learning-in-computer-vision (end)

  % section background (end)

  
  % section section name (end)

  \section{Methodology} % (fold)
  \label{sec:methodology}

  Within this study, we will investigate the use of deep learning techniques for
  indoor localisation when framed as a classification task. We differentiate
  between two types of problem settings throughout the entire report:

  \begin{enumerate}

    \item \textbf{Single-frame classification}: Given a continuous stream of
      frames, a video, the task is to classify each frame individually.

    \item \textbf{Video classification}: Given a continuous stream of frames, a
      video, the task is to classify fixed-sized clips of the video.

  \end{enumerate}

  % TODO: aggregation of predictions for more robustness?

  The following sections describe the data collection, annotation and
  pre-processing, the models used for the two problem settings and the training
  and evaluation procedures.
  
  \subsection{Raw Data} % (fold)
  \label{sub:raw-data}

  Framing the problem of indoor localisation as a classification task requires a
  labelled data set, which consists of sequentially-arranged pairs of inputs and
  outputs, that map visual information to location labels. Both problem settings
  are based on the same raw data, which is described in the following.

  The raw data was collected from a single camera of a mobile device at a frame
  rate of 30 FPS) in high resolution ($2426\times 1125$). The mobile device was
  was hand-held by a human agent while walking around the main building of the
  Southern Campus of the Copenhagen University (Danish: K\o{}benhavns
  Universitet, KU) in Copenhagen S, Denmark. The building is a large
  multi-storey building with a total of six floors, and is used for teaching and
  research purposes. The location was deemed compatible with this study, as it
  showcases distinctive learnable indoor features (e.g. coloured walls, unique
  structures, etc.), but also challenges the model, for example, due to the
  similarity of the floor plan across floors. For the scope of this project, the
  data collection was limited to the first two floors. This process yielded a
  set of videos $V = \{v_1, ..., v_n\}$, where each video $v_i$ is a sequence of
  $k$ frames $f_0, ..., f_k$. Each frame $f_i$ is a RGB image, represented as a
  three-dimensional tensor of shape $3 \times h \times w$, where $h$ and $w$ are
  the height and width of the image, respectively. 

  The raw data also consists of a set of location labels $L = \{l_1, ...,
  l_n\}$, where each location label $l_i$ is a scalar value, which identifies
  the location of the agent at the time of the frame. For the scope of this
  project $|L|=20$ different location labels were considered. The location
  labels are identified both with a descriptive name and a unique integer
  identifier and are shown in Figure~\ref{fig:map} alongside the coloured
  regions, which represent are represented by the location labels. Whenever
  possible, the location labels were designed in close correspondence to the
  building's official floor plan.

  % annotation
  Annotations was performed manually by a single human agent on a frame-by-frame
  basis. However, because changes in the location labels only occur at the
  transition of rooms, the annotation process was simplified by annotating the
  starting and ending time stamps of a location label. 

  % figure
  \begin{figure}[ht]
    \centering
    % subfigure
    \begin{subfigure}[b]{.49\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/map-ground-floor.jpg}
      \caption{Ground Floor}
      \label{fig:map-ground-floor}
    \end{subfigure}
    \hfill
    % subfigure
    \begin{subfigure}[b]{0.49\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/map-first-floor.jpg}
      \caption{First Floor}
      \label{fig:map-first-floor}
    \end{subfigure}
    \caption{
      \textbf{Floor Plan of the Southern Campus of the Copenhagen with Location
      Labels.} The coloured regions represent the $|L|=20$ location labels as
      distributed over the two floors in the indoor space. It is apparent that
      a) the floor plan is similar across floors and b) that locations
      significantly differ in size.
    }
    \label{fig:map}
  \end{figure}

  
  % statistics of the data
  A total of $n=53$ videos of varying length were recorded, with an average
  duration of $\sim 57$s, amounting to a total number of $\sim 50$ minutes of
  footage, or an equivalent of $\sim 90K$ frames. Out of the total 53 videos
  that were recorded, 37 were used for training and 16 were used for testing.
  Importantly, the videos in the training split were recorded in a single
  session, while the videos in the test split were recorded on four separated
  days, in a span of two to four weeks after the training data had been
  recorded. This ensured that the trained model could be tested against unseen
  data, to more accurately assess its generalisation capabilities.

  % \begin{table}[ht]
  %   \centering
  %   \begin{tabular}{lccc}
  %   \toprule
  %    & Total Videos & Total Minutes & Total Frames \\
  %   \midrule
  %   Train & 37 &  37 & 67,200 \\
  %   Test  & 16 & 13 & 23,490 \\
  %   \midrule
  %   Total & \bfseries 53 & \bfseries 50 & \bfseries 90,690 \\
  %   \bottomrule
  %   \end{tabular}
  %   \caption{
  %     \textbf{Statistics of the Raw Data} The raw data consists of a total of 53
  %     videos, of which 37 were used for training and 16 for testing. The total
  %     duration of the raw data is 50 minutes, which amounts to a total of 90,690
  %     frames at a frame rate of 30 FPS.
  %   }
  %   \label{tab:raw-data}
  % \end{table}

  % TODO: figure about the time of data collection (side-by-side, one where hue
  %       denotes the split and the other where hue denotes the clips per day)

  % subsection data-collection (end)
  
  \subsection{Processed Data} % (fold)
  \label{sub:processed-data}

  A video classification models expects a single clip $c_i$ as input, which is a
  sequence of $s_v$ frames sampled from a video $v_i$ at a sampling rate $r_v$.
  . The number of frames per clip $s_v$ and the sampling rate $r_v$ are tied to
  the model architecture and are therefore fixed for each video classification
  model (Detail in Section~\ref{sub:models}). Clips are extracted in uniformly
  spaced intervals. This means that the number of clips $k_v$ that can be
  extracted from a video $v_i$ with $k$ frames is $\left\lfloor k / (r_v \cdot
  s_v) \right\rfloor$. The trailing frames that cannot be extracted as a clip
  are discarded. Each of the $k_v$ clips contains $s_v$ frames, leading to a
  total of $\left\lfloor k / (r_v \cdot s_v) \right\rfloor \cdot s_v$ frames for
  a single video. 

  Single-frame classification models expect a single frame $f_i$ from a video
  $v_i$ as input. Technically, all frames in a video $v_i$ could be used as
  input, but because of the strong local correlation between adjacent frames, it
  was hypothesised that models would overfit to the training data. For this
  reason, and in an attempt to make the training procedure more similar to the
  video classification models, a frame $f_i$ is sampled from a video $v_i$ at a
  sampling rate $r_f$. Here, $r_f$ is not tied to the model architecture and can
  be chosen freely. For all experiments $r_f$ was chosen to be $30$ for all
  single-frame classification, meaning only every 30th frame was sampled from a
  video. The number of frames $k_f$ that can be extracted from a video $v_i$
  with $k$ frames is $\left\lfloor k / r_f \right\rfloor$. It becomes clear that
  the frames considered per video $k_f$ is similar to the number of frames
  considered as part of clips if the two sampling rates $r_v$ and $r_f$ are
  equal, as

  \[
    \left\lfloor \frac{k}{r_v \cdot s_v} \right\rfloor \cdot s_v \approx
    \left\lfloor \frac{k}{r_f} \right\rfloor, \quad \text{if} \quad r_v = r_f
  \]

  The extraction of clips and frames for video and single-frame classification
  models, respectively, is illustrated in Figure~\ref{fig:data-extraction}. It
  shows that despite the same raw data source, the frames considered in the two
  datasets are not the same. This is because the frames in the video dataset are
  potentially sampled at a different rate than the frames in the single-frame
  dataset and because adjacent clips reset the sampling rate. However, it is
  hypothesised that due to large number of total frames and strong local
  correlation between adjacent frames, the two datasets are sufficiently similar
  to allow for comparison between the two model types.

  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/data-extraction.png}
    \caption{
      \textbf{Data Extraction.} A raw video with $k=10$ frames is processed into
      $k_f=5$ frames for the single-frame dataset and $k_v=2$ clips for the
      video dataset. Samples for each dataset type are indicated by a
      surrounding box. Frames are sampled at a rate of $r_f=2$ for the
      single-frame dataset and $r_v=3$ for the video dataset. A clip contains
      $s_v=2$ frames. Frames are colour-coded in correspondence to the dataset
      they belong to (single-frame dataset in purple and video dataset in blue),
      if they are sampled to the respective dataset. Discarded frames are
      coloured in grey (light-grey for within video frames, dark-grey for
      trailing frames).}
    \label{fig:data-extraction}
  \end{figure}
  
  Both image and video classification models expect a single sample $x_i$,
  whether it be a frame or a clip, to be standardised and of a certain size.
  Standardisation is performed on a per-channel basis after normalising the
  input to the range $[0, 1]$. Then, a sample $x_i$ is standardised as,

  \[
    x_{ic}' = \frac{x_{ic} - \mu_c}{\sigma_c} \quad \text{for} \quad c \in \{1, 2, 3\}
  \]

  , where $x_{ic}$ represents the channel $c$ of the sample $x_i$ and $x_{ic}'$
  is the standardised channel $c$ of the sample $x_i$. $\mu_c$ and $\sigma_c$ are
  taken from the ImageNet dataset~\cite{imagenet} for image classification
  models and from the Kinetics dataset~\cite{kinetics} for video 

  The height and width of each input frame is tied to the model architecture and
  is therefore fixed for each model. Input sizes are typically square and range
  from $182 \times 182$ to $224 \times 224$ for all models (Detail in
  Section~\ref{sub:models}). Resizing is performed in a non aspect-preserving
  manner to compress as much of the original viewport as possible into the
  resized frame. However, this also means that the trained models are likely to
  only perform well on frames with a similar raw aspect ratio as the training
  data (at least should be in portrait model).

  After all processing steps, the single-frame dataset contains $n_f$ samples
  and the video dataset contains $n_v$ samples. The number of samples $n_f$ and
  $n_v$ is dependent on the number of videos $n$ and the number of frames $k$ in
  each video. The final single-frame dataset can be written as $D_F = \{(f_1,
  y_1), ..., (f_{n_f}, y_{n_f})\}$, and the final video dataset as $D_V =
  \{(c_1, y_1), ..., (c_{n_v}, y_{n_v})\}$.

  % subsection data-preprocessing (end)


  \subsection{Models} % (fold)
  \label{sub:models}

  A deep learning model is supposed to learn a function $f: X \rightarrow Y$,
  where $X$ represents the input space and $Y$ the output space. In both
  single-frame and video classification, the output space is the discrete set of
  location labels, $Y = L = \{l_1, \ldots, l_n\}$. The input space $X$ is
  different for the two model types, as described in
  Section~\ref{sub:processed-data}. 

  A total of 10 different models are trained and evaluated in this work. The
  models are split into two categories: single-frame classification models and
  video classification models. The single-frame classification models are
  trained on the single-frame dataset $D_F$ and the video classification models
  on the video dataset $D_V$. A comprehensive overview of the models is given in
  Table~\ref{tab:model-overview}. The models are described in detail in the
  following.

  \begin{table}[ht]
    \centering
    \begin{tabular}{cllllllll}
      \toprule
      & \multirow{2}{*}{\textbf{Model}} 
      & \bfseries Release & \bfseries Rate & \bfseries F/C & \bfseries Size &
      \bfseries Params & \bfseries FLOPs & \bfseries Acc@1 \\
      & & (Y) & ($r_f$/$r_v$) & ($s_v$) & ($h$, $w$) & (M) & (G) & (\%) \\
      \midrule
    \multirow{9}{*}{\rotatebox[origin=c]{90}{Single-Frame}} 
    & AlexNet~\cite{alexnet} & 2012 & 30 & - & 224 & 61.1 & 0.71 & 56.52 \\
    & GoogLeNet~\cite{googlenet} & 2014 & 30 & - & 256 & 6.6 & 1.5 & 69.78 \\
    & ResNet18~\cite{resnet} & 2015 & 30 & -  & 224 & 11.7 & 1.82 & 82.52 \\
    & ResNet50~\cite{resnet} & 2015 & 30 & -  & 224 & 25.6 & 4.09 & 80.86 \\
    & DenseNet 121~\cite{densenet} & 2016 & 30 & - & 224  & 7.0 & 2.88 & 74.43 \\
    & MobileNet V3~\cite{mobilenetv3} & 2019 & 30 & - & 224  & 3.5 & 0.32 & 71.88 \\
    & ViT-B-16~\cite{vit} & 2020 & 30 & - & 224  & 86.7 & 17.56 & 81.07 \\
    & EfficientNet V2 S~\cite{efficientnetv2} & 2021 & 30 & - & 224  & 21.5 & 8.37 & 84.23 \\
    & ConvNext Tiny~\cite{convnext} & 2022 & 30 & - & 224  & 28.2 & 4.46 & 82.52 \\
    \midrule
    \multirow{2}{*}{\rotatebox[origin=c]{90}{Video}}
    & R(2+1)D~\cite{r2plus1d} & 2020 & 4 & 16 & 182 & 28.11 & 76.45 & 76.01 \\
    & X3D S~\cite{x3d} & 2020 & 6 & 13 & 3.5 & 182 & 2.96 & 73.33 \\
    \bottomrule
    \end{tabular}
    \caption{
      \textbf{Model Overview.} The table shows all models that were evaluated in
      this work. The models are split into two categories: single-frame models
      and video models. For each model, the table reports the release year
      (Release), the frame rate (Rate) of the training data, the number of
      frames per clip (F/C; \textit{only applicable to video classifiers}), the
      spatial resolution (Size) of the input images, the number of parameters in
      millions (Params), the number of floating point operations in billions
      (FLOPs) and the Top-1 accuracy (Acc\@1) on ImageNet~\cite{imagenet} for
      single-frame classification models and the top-1 accuracy on the
      Kinetics~\cite{kinetics} dataset for video classification models. The
      table is sorted by release date within each group.
    }
    \label{tab:model-overview}
  \end{table}

  \textbf{Alexnet} is a convolutional neural network that was introduced by
  Krizhevsky et al.~\cite{alexnet} in 2012. It was the first deep neural network
  to win the ImageNet Large Scale Visual Recognition Challenge~\cite{imagenet}
  and is considered to be one of the first successful application of deep
  learning to image classification. Architecturally, it uses the standard
  building blocks of convolutional nets. It consists of 5 convolutional layers
  (with filter sizes of $11 \times 11$, $5 \times 5$ and $3 \times 3$) with
  occasional max-pooling layers in between, followed by 3 fully connected
  layers. The network uses the ReLU activation function and dropout for
  regularization.

  \textbf{GoogLeNet} is a convolutional neural network that was introduced by
  Szegedy et al.~\cite{googlenet} in 2014. At its core, it is based on Inception
  modules, which allow the network to choose between multiple convolutional
  filter sizes in each block. Each inception module consists of 4 parallel
  convolutional layers, whose outputs are concatenated along the channel
  dimension. This advancement allowed the Google Team to significantly reduce
  the number of parameters in the network and win the ImageNet Large Scale
  Visual Recognition Challenge~\cite{imagenet} in 2014.

  \textbf{ResNet} was introduced by He et al.~\cite{resnet} in 2015. The main
  contribution of ResNet is the residual block, which allows the network to
  learn residual functions with respect to the layer inputs. This allows the
  network to be trained much deeper than previous architectures. The original
  ResNet paper introduced several architectures with different depths. In this
  work, we use ResNet18 and ResNet50, which are among the smallest architectures
  in the family.

  \textbf{Densenet} was introduced by Huang et al.~\cite{densenet} in 2016. The
  main contribution of Densenet is the dense block, which allows the network to
  learn feature maps from all preceding layers. This allows the network to be
  trained much deeper than previous architectures. The original Densenet paper
  introduced several architectures with different depths. In this work, we use
  Densenet121, which is among the smallest architectures in the family.

  \textbf{MobileNet V3} was introduced by Howard et al.~\cite{mobilenetv3} in
  2019. The main contribution of MobileNet V3 is the efficient inverted
  bottleneck block.

  \textbf{ViT} was introduced by Dosovitskiy et al.~\cite{vit} in 2020. It is
  the first attempt to apply the Transformer architecture proposed by Vaswani
  et. al~\cite{transformer} to computer vision task. The main contribution of
  ViT is the patch embedding mechanism, which allows to transform a
  two-dimensional image into a one-dimensional sequence of tokens that,
  alongside a positional encoding, can be processed by the Transformer
  architecture. The original ViT paper introduced several architectures with
  different depths. In this work, we use ViT-B-16, which uses a patch dimension
  of $16 \times 16$.

  \textbf{EfficientNet V2} was introduced by Tan et al.~\cite{efficientnetv2} in
  2021. It is the successor of EfficientNet~\cite{efficientnet}, which was
  introduced in 2019. The main contribution of EfficientNet V2 is the efficient
  inverted bottleneck block, that also underlies MobileNet
  V3~\cite{mobilenetv3}.

  \textbf{ConvNext} is the most recent model in this work. It was introduced by
  Wang et al.~\cite{convnext} in 2021, producing state-of-the-art results.

  \textbf{R(2+1)D} was introduced by Tran et al.~\cite{r2plus1d} in 2018. It is
  the first attempt to apply the ResNet architecture~\cite{resnet} to video
  classification tasks. The main contribution of R(2+1)D is the spatiotemporal
  convolutional block, which allows the network to learn spatiotemporal
  features. The original R(2+1)D paper introduced several architectures with
  different depths. In this work, we use R(2+1)D-18.

  \textbf{X3D} was introduced by Feichtenhofer et al.~\cite{x3d} in 2020.
  \textit{TBD}.

  % cnns
  CNNs are a class of deep neural networks that are applied to analyse visual
  imagery. They are composed of several layers of alternating convolutional and
  pooling filters, which are applied to the input image to extract high-level
  feature representations. These high-level feature representations are then fed
  into a fully connected neural network (FCNN), which outputs a probability
  distribution over the location labels.

  For a video classification model, all $N$ consecutive frames are independently
  passed through a CNN module. The sequence of high-level feature
  representations is then fed into a recurrent neural network (RNN), which
  outputs a probability distribution over the location labels for each frame by
  placing a fully connected layer (classification head) to the output vector
  associated to each frame in the RNN module. 

  % rnns
  A recurrent neural network (RNN)
  is a class of deep neural networks that is applied to analyse sequential data.
  Although they are mostly applied to natural language processing tasks, the
  generality of their architecture allows to also apply them to computer vision
  tasks with sequential data, such as a sequence of frames in video
  classification. For each input $x_i$ in a sequence of inputs, the RNN computes
  an output as a function of the current input $x_i$ and a hidden state
  $h_{i-1}$ that is computed as a function of all previous inputs. In that way,
  the RNN can learn to model the temporal dependencies between frames, e.g.
  consider that all previous frames were predicted to be at one specific
  location, and therefore the current frame is also likely to be at that
  location.

  % subsection models (end)

  \subsection{Training} % (fold)
  \label{sub:training}

  All models were implemented using the PyTorch framework~\cite{pytorch} and the
  training was performed locally on a MacBook Pro M1 with 16GB of memory and
  MPS-acceleration enabled (where possible\footnote{MobileNet-V3-based models do
  not support GPU acceleration}) to allow for fast experiment iteration. The
  loss function $\mathcal{L}$ used for all models was cross-entropy loss
  (Equation~\ref{eq:cross-entropy}), which
  is a standard loss function for multi-class classification problems.

  \begin{equation}
    \mathcal{L}(\hat{y},y) = -\sum_{i=1}^{K} y_i \log(\hat{y}_i)
    \label{eq:cross-entropy}
  \end{equation}

  % TODO: compare to pytorch cross-entropy loss (is it the same? what is the
  % difference to nll?)
  Here, $\hat{y}$ is the predicted probability distribution over the $K$ classes
  and $y$ is the one-hot encoded ground truth label. The models were trained
  using the AdamW~\cite{adamw} optimiser with default parameters, except for the
  learning rate, which was set to a constant of $1e^{-4}$. Step-wise learning
  rate scheduling was used for all models, which reduced the learning rate by a
  factor of 10 every 5 epochs. The batch size was set to 32 for all image
  classifiers and 8 for all video classifiers for memory-optimal training.
  Unless otherwise specified, all image and video classifiers were trained with
  the same set of training hyper-parameters, which are specified in Table
  \ref{tab:default-hyperparams}. 

  \begin{table}[ht]
    \centering
    \begin{tabular}{rllll}
      \toprule
      Classifier & Batch Size & Epochs & Optimiser & Learning Rate Scheduler \\
      \midrule
      \bfseries Image & 32 & 10 & AdamW ($\gamma=1e^{-4}$) & Step-LR
      ($\gamma=1e^{-1}, s=5$) \\
      \bfseries Video & 8 & 10 & AdamW ($\gamma=1e^{-4}$) & Step-LR
      ($\gamma=1e^{-1}, s=5$) \\
      \bottomrule
    \end{tabular}
    \caption{Default Hyperparameters for Image and Video Classifiers}
    \label{tab:default-hyperparams}
  \end{table}

  %   (end)

  \subsection{Evaluation} % (fold)
  \label{sub:evaluation}

  The goal of the evaluation is to have a rigorous comparison of the different
  models and to determine the best model for the task of location
  classification. In this context, a ``well-performing'' model is not just the
  model that is most accurate in most cases, but also the model that is most
  robust to a wide-range of different natural variations that can occur and that
  is efficient. To achieve this, a series of \textit{quantitative} and
  \textit{qualitative} experiments are performed on the models, which are then
  used in Section~\ref{sec:results} and Section~\ref{sec:discussion}.

  \textbf{Quantitative Experiments.} To quantitatively assess the performance of
  the model on the task of location classification, the models were evaluated on
  the test set using a wide-range of different performance metrics. To assess
  the overall accuracy of the model, two metrics are computed:
  \textbf{Multi-class Accuracy} (Equation~\ref{eq:accuracy}) is the most
  wide-spread performance metric in classification task and is defined as the
  number of correct predictions divided by the total number of predictions.
  It gives a good first indication for the overall performance of the model, but
  can be misleading in cases where the dataset is imbalanced.

  \begin{equation}
    \text{Multi-class Accuracy} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(y_i =
    \hat{y}_i) 
    \label{eq:accuracy}
  \end{equation}

  Here, $\mathbb{I}$ is an indicator that is 1 if the condition is true and 0
  otherwise. On top of the standard, multi-class accuracy, a variant of the
  formula, the \textbf{Top-3 Multi-class Accuracy} is also computed, which
  counts a prediction as correct if the correct label is one of the top-3
  predicted labels.

  As some location labels are underrepresented in the dataset due to the natural
  variation in size of the different locations, the \textbf{Macro F1-Score} is
  used to compute a more fine-grained metric. The Macro F1-Score is the average
  of the class-specific F1-scores, which are defined as the harmonic mean of
  precision $P_i$ and recall $R_i$ (Equation~\ref{eq:macro-f1}).

  \begin{equation}
    \text{Macro F1-Score} = \frac{1}{N} \sum_{i=1}^{N} \frac{2 \cdot P_i \cdot 
    R_i}{P_i + R_i}
    \label{eq:macro-f1}
  \end{equation}

  Here, $P_i$ and $R_i$ are the precision and recall for class $i$, which are 
  defined as follows (Equation~\ref{eq:precision}) and (Equation~\ref{eq:recall}).

  \begin{equation}
    P_i = \frac{TP_i}{TP_i + FP_i}
    \label{eq:precision}
  \end{equation}

  \begin{equation}
    R_i = \frac{TP_i}{TP_i + FN_i}
    \label{eq:recall}
  \end{equation}

  Here, $TP_i$ is the number of true positives for class $i$, $FP_i$ is the
  number of false positives for class $i$ and $FN_i$ is the number of false
  negatives for class $i$. 

  On top of the standard metrics for classification, efficiency of the model is
  crucial for usability of the system on low-resource devices, such as mobile
  phones. For this reason, the \textbf{Inference Time} per sample is measured, 
  the \textbf{GFLOPS} per predicted sample is computed and the \textbf{Model
  Size} is measured to assess the memory constraints imposed by the model.

  \textbf{Qualitative Experiments.} Purely quantitatively assessing a model's
  performance may not be sufficient to truly determine the strengths and
  weaknesses of the model. For this reason, the models were also evaluated
  qualitatively by manually inspecting the misclassified samples and
  the predictions of the models on a sub-set of $20$ test frames. 
  Furthermore, the \texttt{GradCam}~\cite{gradcam} algorithm was used to gain 
  insights into the internal functioning of the model. GradCam is a technique
  that backtracks the activations in the convolutional filters of the model at
  some depth in the model's architecture and through the gradients of the model
  to the input image. This allows to visualise the regions of the image that
  were most relevant for the model to make its prediction. In this scenario, it
  is hoped that the highlighted regions can be used to understand what types of
  features the model is looking for in the image and what types of features it
  is not looking for. This can be used to identify potential issues with the
  model and to gain insights into how the model can be improved.

  % subsection evaluation (end)

  % section methodology (end)


  \section{Results} % (fold)
  \label{sec:results}

  % computer vision models are capable of solving indoor localisation when
  % phrased as a classification task

  The series of experiments conducted within this study show that computer
  vision models, when carefully designed and trained, are generally capable of
  solving the task of indoor localisation when phrased as a coarse-grained
  classification problem. Table~\ref{tab:results-experiment1} shows the results
  of the performance and efficiency metrics of all models (see
  Table~\ref{tab:models}).

  \begin{table}[ht]
    \centering
    \begin{tabular}{clllllll}
      \toprule
      & \multirow{2}{*}{\textbf{Model}} 
      & \textbf{1-Acc.} & \textbf{3-Acc.}& \textbf{Ma.-F1} & \textbf{Par.} & \textbf{FLOPs} & \textbf{Throughput} \\
      & & (\%) & (\%) & (\%) & (M) & (G) & (Preds/s) \\
      \midrule
    \multirow{4}{*}{\rotatebox[origin=c]{90}{Image}} &
        \texttt{alexnet} & 60.15 & 84.42 & 55.95 & 57.09 & 0.71 & \bfseries
        87.31 ($\pm$ 1.15) \\
      & \texttt{resnet18} & 71.39 & 91.70 & \bfseries 68.29 & 11.19 & 1.82 &
      64.12 ($\pm$ 1.90) \\
      & \texttt{resnet50} & 58.37 & 82.63 & 55.92 & 23.55 & 4.12 & 27.57 ($\pm$ 1.10) \\
      & \texttt{mobilenet\_v3} & 29.37 & 64.11 & 14.87 & 1.54 & 0.06 &
      16.49 ($\pm$ 1.44) \\

      \midrule

      \multirow{4}{*}{\rotatebox[origin=c]{90}{Video}}
      & \texttt{alexnet-lstm} & 32.60 & 62.19 & 9.95 & \bfseries 58.58 & 3.57 &
      38.31 ($\pm$ 0.53) \\
      & \texttt{alexnet-rnn} & 50.00 & 78.90 & 35.71 & 58.19 & 3.56 & 39.09
      ($\pm$ 0.51) \\
      & \texttt{resnet18-lstm} & \bfseries 72.19 & 91.78 & 64.59 & 11.84 &
        \bfseries 9.11 & 19.66 ($\pm$ 0.40) \\
      & \texttt{resnet18-rnn} & 70.14 & \bfseries 93.97 & 66.62 & 11.44 & 9.11 &
      19.07 ($\pm$ 0.56) \\

      \bottomrule
    \end{tabular}

    \caption{
      \textbf{Results.} The table shows a series of performance and efficiency
      metrics for the models trained in the first experiment. Here,
      \texttt{1-Acc} and \texttt{3-Acc} refer to the Top-1 and Top-3 Accuracy
      scores respectively, \texttt{Ma.-F1} refers to the Macro-F1 score,
      \texttt{Par.} refers to the number of parameters in the model, \texttt{FLOPs}
      refers to the number of floating point operations required to make a
      prediction and \texttt{Throughput} refers to the number of predictions
      that can be made per second. The values for \texttt{Throughput} are
      averaged over $10$ runs and the standard deviation is reported in
      parentheses. The best performing model for each metric is highlighted in
      bold.
    }

    \label{tab:results-experiment1}
  \end{table}

  \subsection{Detailed Analysis} % (fold)
  \label{sub:Detailed Analysis}

  The best performing model in terms of classification performance are the
  Resnet18-based models. The pure CNN module reaches a maximal Macro F1-Score 
  of 68.29\% and a Top-1 Accuracy of 71.39\%. Stacking a RNN leads to slight
  decreases in the performance (-1.67\%, -1,25\%), while stacking a LSTM layer
  leads to slight increases in Top-1 Accuracy (+0.8\%) and a more significant
  decrease in Macro-F1 (-3,7\%). ResNet50 performs worse than ResNet18, with a
  maximal Macro-F1 of 55.92\% and a Top-1 Accuracy of 58.37\%. The increased
  number of parameters does not seem to be necessary given the complexity (and
  small size) of the dataset.
  Despite the higher number of parameters, AlexNet performs worse than ResNet18,
  with a maximal Macro-F1 of 55.95\% and a Top-1 Accuracy of 60.15\%. The
  architectural differences, specifically the lack of residual connections, are
  hypothesised to be the major cause for the drop in performance. Interestingly,
  by stacking a RNN or LSTM layer on top of the AlexNet module, classification
  performance drops sharply. With a RNN, the Macro F1-Score drops to 35,71\%
  (-20.24\%) and with a LSTM, the Macro F1-Score drops to 9.95\% (-46.00\%),
  barely better than a random guess.
  MobileNet-V3 performs the worst, with a maximal Macro-F1 of 14.87\% and a
  Top-1 Accuracy of 29.37\%. Given the significantly lower number of parameters 
  (1.54M), the model does not seem complex enough to learn meaningful features
  in the dataset.

  \begin{figure}[ht]
    \begin{center}
      \includegraphics[width=0.65\textwidth]
      {./figures/itu.jpg}
    \end{center}

    \caption{
    \textbf{Performance Metrics.} The three classification performance metrics
    are shown for all models in a grouped bar chart.
    }

    \label{fig:performance-metrics}
  \end{figure}

  \subsection{Performance Efficiency Trade-Off} % (fold)
  \label{sub:Performance Efficiency Trade-Off}

  The general trend in deep learning is that increasing the number of
  parameters, and thereby the complexity of the model, generally leads to better 
  performance. In this experiment, higher model complexity was reached in two
  ways: Firstly, by increasing the number of parameters in the CNN module, and,
  secondly, by stacking a RNN or LSTM layer on top of the CNN module.
  In order to investigate the performance-efficiency trade-off, the number of
  parameters, the number of floating point operations and the throughput of all
  models were measured and plotted against the Top-1 Test Accuracy. The results
  are shown in Figure~\ref{fig:performance-efficiency-trade-off}.

  Figure~\ref{fig:performance-efficiency-trade-off} shows the Top-1 Test
  Accuracy of all models against the number of parameters.
  
  \begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{./figures/itu.jpg}

    \caption{
      \textbf{Performance-Efficiency Trade-Off.} The three Figures show the
      Top-1 Accuracy (\%) against (a) the number of parameters, (b) the number
      of floating point operations and (c) the throughput (predictions per
      second) for all models. The models are coloured according to their
      CNN base module and the marker style indicates the type of RNN head.
      The grey lines aid the visualisation of the development of
      performance-efficiency trade-off with different recurrent modules.
    }

    \label{fig:performance-efficiency-trade-off}
  \end{figure}

  It can be seen that higher model complexity generally does not lead to higher
  performance. Both ResNet50 and AlexNet have significantly more parameters than
  ResNet18, but perform worse. The picture is slightly different for the number
  of floating points operations and throughput. As expected, ResNet50, due to
  its higher number of parameters, requires more floating point operations
  during inference, which leads to a lower throughput. However, AlexNet, despite
  being almost 6x larger than ResNet18 in terms of pure parameter count, has a
  lower number of floating point operations and a higher throughput. However,
  since the base ResNet18 model already has a throughput of ~65 frames per
  second, which is more frames per second than most modern cameras capture, the
  throughput does not seem to be a limiting factor.

  % subsection subsection name (end)
  
  % subsection subsection name (end)

  \subsection{Temporal Dimension} % (fold)
  \label{sub:Temporal Dimension}

  One of the main difference in the trained models is whether or not they are
  capable of processing the temporal nature of the video frames. It was
  hypothesised that the CNN-RNN video classification models would perform better
  due to an increased number of features and the ability to learn temporal
  information from the video frames. The results show that this is not the case.

  Figure~\ref{fig:performance-efficiency-trade-off} not only shows the
  performance-efficiency trade-off, but also the difference in performance and
  efficiency between the base CNN modules and the CNN-RNN models. The grey lines
  connect the base CNN module with the CNN-RNN model. It can be seen that adding
  the RNN head leads to unchanged (ResNet18) or decreased (AlexNet) performance,
  and significantly higher number of floating point operations and therefore
  lower throughput. The analysis clearly shows that adding a RNN head does not
  yield the desired effect of modelling the temporal dimension of the video
  frames to increase the robustness of continuous predictions. Instead, it
  merely increases the model complexity and inference time, which make it
  overall inferior to their pure CNN counterparts.

  \subsection{Data Efficiency} % (fold)
  \label{sub:Data Efficiency}

  Approaching indoor localisation as a classification task, means to frame it as
  a supervised learning problem. Supervised learning requires human-annotated
  training data, which can be prohibitive to obtain in large quantities. For the
  proposed approach to be valid, minimal effort in the initial data collection
  and annotation should be required. For this reason, the second experiment
  tries to find an answer for the research question: \textit{ ``How much data
  is necessary to achieve good performance''}. To answer this question, a single
  image and video classifier are chosen and trained on different subsets of the 
  training data. The subsets include $10\%$, $20\%$, $30\%$, $40\%$, $50\%$,
  $60\%$, $70\%$, $80\%$, $90\%$ and $100\%$ of the training data. The models 
  are then evaluated as outlined in Section~\ref{sub:evaluation} to find the
  natural drop-off in performance as the amount of training data decreases.


  % data-efficient learners (figure: ratio-vs-performance)

  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{./figures/itu.jpg}
    \caption{
      \textbf{Data Efficiency.} The figure shows the performance of the baseline
      model \texttt{resnet18} when trained on different training data metrics.
    }
    \label{fig:data-efficiency}
  \end{figure}
  
  
  % subsection subsection name (end)

  
  
  % subsection subsection name (end)
  
  \subsection{Analysing Failing Scenarios: Mispredicted Samples} % (fold)
  \label{sub:Analysing Failing Scenarios: Mispredicted Samples}

  To better understand the strengths and weaknesses of the best-performing
  model, a pure ResNet CNN module, the confusion matrix is analysed.
  Figure~\ref{fig:resnet18-confusion-matrix} shows the confusion matrix of the
  ResNet18 model.

  \begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{figures/itu.jpg}
      \caption{Unnormalised Confusion Matrix}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{figures/itu.jpg}
      \caption{Row-Normalised Confusion Matrix}
    \end{subfigure}
    \caption{
      \textbf{ResNet18 Confusion Matrix.} The figure shows the (a) unnormalised
      and (b) row-normalised confusion matrix. For (a), the entry at row $i$ and
      column $j$ shows the number of samples that belong to class $i$ but were
      predicted to be class $j$. For (b), the entry at row $i$ and column $j$
      shows}
    \label{fig:resnet18-confusion-matrix}
  \end{figure}
  
  % subsection Mispredicted Samples (end)

  \subsection{Extracting Characteristics of the Model using GradCam} % (fold)
  \label{sub:Mispredicted Samples}

  % qualitative characteristics of resnet18: gradcam

  % subsection Mispredicted Samples (end)

  \subsection{Deployment on Mobile Devices}
  \label{sub:deploment}

  As a proof of concept, the best-performing model, was deployed on a mobile
  device. The trained model was first quantised to 8-bit float precision and
  then converted to \texttt{TorchScript} format. The \texttt{TorchScript} model
  was then deployed on a mobile device using the \texttt{PlayTorch}
  framework~\textit{[CITE]}. The \texttt{PlayTorch} framework is a port of the
  PyTorch Mobile SDK that is natively compatible with native iOS and Android
  Deployment to Javascript. This allows for rapid prototyping and deployment of
  PyTorch models on mobile device using the popular React Native framework and
  Expo. The model runs locally on the device and therefore does not require any
  internet connection after initially downloading the model upon opening the
  app.

  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{./figures/playtorch-qr.png}
    \caption{
      \textbf{PlayTorch App.} Download the PlayTorch iOS or Android app and scan
    the QR code to try out the model on your mobile device.}
    \label{fig:playtorch-qr}
  \end{figure}

  % subsection deployment (end)

  % section Results (end)

  \section{Limitations \& Future Work} % (fold)
  \label{sec:discussion}

  This study has shown the potential of using a pure deep learning pipeline for
  tackling the problem of indoor localisation. However, there are still many
  open questions that need to be addressed in future work for systems similar to
  the approach suggested here to be widely useful in real-world applications.

  % coarse location labels
  The main drawback of the proposed approach is the coarse location labels.
  State-of-the-art indoor localisation systems are able to localise users to
  centimetre accuracy. Therefore, future work should focus making improvements
  on the entire pipeline that allow for higher precision in the location labels.

  % small data set
  Furthermore, this study assumes a small data set of only 40 minutes of video
  footage for training. While the experiment setup allowed to draw conclusion
  about the data efficiency of the models and the general feasibility of the
  data collection and annotation process, it is an open question how similar
  systems scale to even larger indoor spaces with more training data available.

  % representativeness of test split
  Additionally, it has to be noted that the test split was collected over a
  duration of four different days in close succession. Therefore, the test split
  might not be representative of the true variation of visual inputs from a
  indoor location over the course of a year. For example, the location might
  change more significantly than represented in the test split over seasons.
  In that case, the reported test metrics are likely to be over-optimistic. To
  gain confidence in favour or against this hypothesis, monitoring the
  performance of the trained models on test data collected over a longer period
  of time would be necessary.

  % edge-cases
  Finally, the detailed analysis of the mispredicted samples has shown that
  most errors, because the models that show a lot of similar features, such as
  different libraries, or rooms that are architecturally similar across floors.
  Future work should specifically improve on finding solutions to these issues.
  Possible starting points might be to use the fact that sudden jumps in the
  building are impossible, so highly confident predictions from the past can be
  used as a strong indicator for the next room if some knowledge about the
  relative position of the rooms is available.

  % section Discussion (end)

  \section{Conclusion} % (fold)
  \label{sec:conclusion}

  We have shown that the problem of indoor localisation can be phrased as a
  classification problem and be solved through the use of deep learning models
  in a data-efficient manner.
  Simple convolutional neural architectures were able to provide accurate and
  robust predictions and showed emerging learning of features mostly invariant
  to natural variations in the environment.
  The use of a recurrent neural network architecture allowed for the
  incorporation of temporal information and improved the accuracy of the
  predictions, though only marginally so.

  % section conclusion (end)

  % bibliography
  \newpage
  \bibliography{references}
  \bibliographystyle{plain}


  \newpage
  \section{Appendix} % (fold)
  \label{sec:appendix}

  \subsection{Reproducibility} % (fold)
  \label{sub:reproducibility}

  All code and data used in this project is available on GitHub at the
  following link: \url{https://github.com/mikasenghaas/bsc}. The project's 
  \texttt{README} file contains detailed instructions on how to reproduce the
  results of this project.

  Further, the precise configuration and results of the experiments that are
  reported here are publicly available on the Weights \& Biases platform at the
  following link: \url{https://wandb.ai/mikasenghaas/bsc}.

  % subsection reproducibility (end)

  \subsection{Machine Specifications} % (fold)
  \label{sub:machine-specs}

  Table~\ref{tab:machine-specs} lists the specifications of the machine that was
  used for training and evaluation of the models. For training the \texttt{MPS}
  backend was used for accelerated training. However, for inference a single CPU
  core was used, to approximate latency and throughput on mobile devices.


  \begin{table}[ht]
    \centering
    \begin{tabular}{cll}
     \toprule
     & Specification & Value \\
     \midrule

     \multirow{2}{*}{System} & Name & Darwin \\
     \vspace{0.1cm}
     & Node & Local MacBook Pro \\

     \multirow{4}{*}{CPU} & Model & Apple M1 \\
     & Architecture & ARM64 \\
     & Physical Cores & 8 \\
     \vspace{0.1cm}
     & Frequency & 2.4 GHz \\

     \multirow{2}{*}{Memory} & Total Capacity & 16 GB \\
     & Avg. Used Capacity & $\sim 7.4$ GB \\

     \bottomrule
    \end{tabular}
    \caption{Machine Specifications}
    \label{tab:machine-specs}
  \end{table}
  
  % subsection subsection name (end)

  % section remarks (end)

  % conf matrices for all models

  \newpage
  \restoregeometry

  % encoding 
  \begin{table}[ht]
    \centering
    \begin{tabular}{lc}
    \toprule
    \bfseries Class & \bfseries Encoding \\
    \midrule
    First\_Floor\_Corridor\_1 & A \\
    First\_Floor\_Corridor\_2 & B \\
    First\_Floor\_Green\_Area & C \\
    First\_Floor\_Library\_1 & D \\
    First\_Floor\_Library\_2 & E \\
    First\_Floor\_Library\_3 & F \\
    First\_Floor\_Magenta\_Area & G \\
    First\_Floor\_Mezzanine & H \\
    First\_Floor\_Red\_Area & I \\
    First\_Floor\_Yellow\_Area & J \\
    Ground\_Floor\_Atrium & K \\
    Ground\_Floor\_Corridor\_1 & L \\
    Ground\_Floor\_Corridor\_2 & M \\
    Ground\_Floor\_Entrance\_Magenta & N \\
    Ground\_Floor\_Entrance\_Yellow & O \\
    Ground\_Floor\_Green\_Area & P \\
    Ground\_Floor\_Magenta\_Area & Q \\
    Ground\_Floor\_Red\_Area & R \\
    Ground\_Floor\_Yellow\_Area & S \\
    Stairs\_Atrium & T \\
    \bottomrule
    \end{tabular}
    \caption{Encoding of Classes}
    \label{tab:class-encoding}
  \end{table}

  % section appendix (end)
  

\end{document}
